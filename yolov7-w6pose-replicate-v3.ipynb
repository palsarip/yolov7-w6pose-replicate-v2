{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918abb4c-de1d-4a2a-b13e-6a45c8abd45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f91622-bc34-4823-bf02-e58ef1c8b4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to weights file [default: yolov7-w6-pose.pt]:  \n",
      "Use GPU? (y/n) [default: y]:  \n",
      "Enter GPU device ID [default: 0]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select input source:\n",
      "1: Video file\n",
      "2: Webcam\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice [1/2]:  1\n",
      "Enter video file path [default: sample_video.mp4]:  datasets/le2i/Le2i_Sorted/Fall/Coffee_room_02/Videos/video (49).avi\n",
      "Display video with pose estimation in real-time? (y/n) [default: y]:  y\n",
      "Save output video? (y/n) [default: y]:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running fall detection with:\n",
      "- Weights: yolov7-w6-pose.pt\n",
      "- Device: 0\n",
      "- Source: datasets/le2i/Le2i_Sorted/Fall/Coffee_room_02/Videos/video (49).avi\n",
      "- Display: Yes\n",
      "- Save output: Yes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Confirm? (y/n) [default: y]:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from yolov7-w6-pose.pt, 161.1MB\n",
      "Initializing Fall Detector with weights: yolov7-w6-pose.pt on device: 0\n",
      "Fusing layers... \n",
      "Output will be saved to: output\\video (49)_fall_detection.mp4\n",
      "Starting fall detection...\n",
      "Processing frame 1\n",
      "Processing frame 2\n",
      "Processing frame 3\n",
      "Processing frame 4\n",
      "Processing frame 5\n",
      "Processing frame 6\n",
      "Processing frame 7\n",
      "Processing frame 8\n",
      "Processing frame 9\n",
      "Processing frame 10\n",
      "Processing frame 11\n",
      "Processing frame 12\n",
      "Processing frame 13\n",
      "Processing frame 14\n",
      "Processing frame 15\n",
      "Processing frame 16\n",
      "Processing frame 17\n",
      "Processing frame 18\n",
      "Processing frame 19\n",
      "Processing frame 20\n",
      "Processing frame 21\n",
      "Processing frame 22\n",
      "Processing frame 23\n",
      "Processing frame 24\n",
      "Processing frame 25\n",
      "Processing frame 26\n",
      "Processing frame 27\n",
      "Processing frame 28\n",
      "Processing frame 29\n",
      "Processing frame 30\n",
      "Processing frame 31\n",
      "Processing frame 32\n",
      "Processing frame 33\n",
      "Processing frame 34\n",
      "Processing frame 35\n",
      "Processing frame 36\n",
      "Processing frame 37\n",
      "Processing frame 38\n",
      "Processing frame 39\n",
      "Processing frame 40\n",
      "Processing frame 41\n",
      "Processing frame 42\n",
      "Processing frame 43\n",
      "Processing frame 44\n",
      "Processing frame 45\n",
      "Processing frame 46\n",
      "Processing frame 47\n",
      "Processing frame 48\n",
      "Processing frame 49\n",
      "Processing frame 50\n",
      "Processing frame 51\n",
      "Processing frame 52\n",
      "Processing frame 53\n",
      "Processing frame 54\n",
      "Processing frame 55\n",
      "Processing frame 56\n",
      "Processing frame 57\n",
      "Processing frame 58\n",
      "Processing frame 59\n",
      "Processing frame 60\n",
      "Processing frame 61\n",
      "Processing frame 62\n",
      "Processing frame 63\n",
      "Processing frame 64\n",
      "Processing frame 65\n",
      "Processing frame 66\n",
      "Processing frame 67\n",
      "Processing frame 68\n",
      "Processing frame 69\n",
      "Processing frame 70\n",
      "Processing frame 71\n",
      "Processing frame 72\n",
      "Processing frame 73\n",
      "Processing frame 74\n",
      "Processing frame 75\n",
      "Processing frame 76\n",
      "Processing frame 77\n",
      "Processing frame 78\n",
      "Processing frame 79\n",
      "Processing frame 80\n",
      "Processing frame 81\n",
      "Processing frame 82\n",
      "Processing frame 83\n",
      "Processing frame 84\n",
      "Processing frame 85\n",
      "Processing frame 86\n",
      "Processing frame 87\n",
      "Processing frame 88\n",
      "Processing frame 89\n",
      "Processing frame 90\n",
      "Processing frame 91\n",
      "Processing frame 92\n",
      "Processing frame 93\n",
      "Processing frame 94\n",
      "Processing frame 95\n",
      "Processing frame 96\n",
      "Processing frame 97\n",
      "Processing frame 98\n",
      "Processing frame 99\n",
      "Processing frame 100\n",
      "Processing frame 101\n",
      "Processing frame 102\n",
      "Processing frame 103\n",
      "Processing frame 104\n",
      "Processing frame 105\n",
      "Processing frame 106\n",
      "Processing frame 107\n",
      "Processing frame 108\n",
      "Processing frame 109\n",
      "Processing frame 110\n",
      "Processing frame 111\n",
      "Processing frame 112\n",
      "Processing frame 113\n",
      "Processing frame 114\n",
      "Processing frame 115\n",
      "Processing frame 116\n",
      "Processing frame 117\n",
      "Processing frame 118\n",
      "Processing frame 119\n",
      "Processing frame 120\n",
      "Processing frame 121\n",
      "Processing frame 122\n",
      "Processing frame 123\n",
      "Processing frame 124\n",
      "Processing frame 125\n",
      "Processing frame 126\n",
      "Processing frame 127\n",
      "Processing frame 128\n",
      "Processing frame 129\n",
      "Processing frame 130\n",
      "Processing frame 131\n",
      "Processing frame 132\n",
      "Processing frame 133\n",
      "Processing frame 134\n",
      "Processing frame 135\n",
      "Processing frame 136\n",
      "Processing frame 137\n",
      "Processing frame 138\n",
      "Processing frame 139\n",
      "Processing frame 140\n",
      "Processing frame 141\n",
      "Processing frame 142\n",
      "Processing frame 143\n",
      "Processing frame 144\n",
      "Processing frame 145\n",
      "Processing frame 146\n",
      "Processing frame 147\n",
      "Processing frame 148\n",
      "Processing frame 149\n",
      "Processing frame 150\n",
      "Processing frame 151\n",
      "Processing frame 152\n",
      "Processing frame 153\n",
      "Processing frame 154\n",
      "Processing frame 155\n",
      "Processing frame 156\n",
      "Processing frame 157\n",
      "Processing frame 158\n",
      "Processing frame 159\n",
      "Processing frame 160\n",
      "Processing frame 161\n",
      "Processing frame 162\n",
      "Processing frame 163\n",
      "Processing frame 164\n",
      "Processing frame 165\n",
      "Processing frame 166\n",
      "Processing frame 167\n",
      "Processing frame 168\n",
      "Processing frame 169\n",
      "Processing frame 170\n",
      "Processing frame 171\n",
      "Processing frame 172\n",
      "Processing frame 173\n",
      "Processing frame 174\n",
      "Processing frame 175\n",
      "Processing frame 176\n",
      "Processing frame 177\n",
      "Processing frame 178\n",
      "Processing frame 179\n",
      "Processing frame 180\n",
      "Processing frame 181\n",
      "Processing frame 182\n",
      "Processing frame 183\n",
      "Processing frame 184\n",
      "Processing frame 185\n",
      "Processing frame 186\n",
      "Processing frame 187\n",
      "Processing frame 188\n",
      "Processing frame 189\n",
      "Processing frame 190\n",
      "Processing frame 191\n",
      "Processing frame 192\n",
      "Processing frame 193\n",
      "Processing frame 194\n",
      "Processing frame 195\n",
      "Processing frame 196\n",
      "Processing frame 197\n",
      "Processing frame 198\n",
      "Processing frame 199\n",
      "Processing frame 200\n",
      "Processing frame 201\n",
      "Processing frame 202\n",
      "Processing frame 203\n",
      "Processing frame 204\n",
      "Processing frame 205\n",
      "Processing frame 206\n",
      "Processing frame 207\n",
      "Processing frame 208\n",
      "Processing frame 209\n",
      "Processing frame 210\n",
      "Processing frame 211\n",
      "Processing frame 212\n",
      "Processing frame 213\n",
      "Processing frame 214\n",
      "Processing frame 215\n",
      "Processing frame 216\n",
      "Processing frame 217\n",
      "Processing frame 218\n",
      "Processing frame 219\n",
      "Processing frame 220\n",
      "Processing frame 221\n",
      "Processing frame 222\n",
      "Processing frame 223\n",
      "Processing frame 224\n",
      "Processing frame 225\n",
      "Processing frame 226\n",
      "Processing frame 227\n",
      "Processing frame 228\n",
      "Processing frame 229\n",
      "Processing frame 230\n",
      "Processing frame 231\n",
      "Processing frame 232\n",
      "Processing frame 233\n",
      "Processing frame 234\n",
      "Processing frame 235\n",
      "Processing frame 236\n",
      "Processing frame 237\n",
      "Processing frame 238\n",
      "Processing frame 239\n",
      "Processing frame 240\n",
      "Processing frame 241\n",
      "Processing frame 242\n",
      "Processing frame 243\n",
      "Processing frame 244\n",
      "Processing frame 245\n",
      "Processing frame 246\n",
      "Processing frame 247\n",
      "Processing frame 248\n",
      "Processing frame 249\n",
      "Processing frame 250\n",
      "Processing frame 251\n",
      "Processing frame 252\n",
      "Processing frame 253\n",
      "Processing frame 254\n",
      "Processing frame 255\n",
      "Processing frame 256\n",
      "Processing frame 257\n",
      "Processing frame 258\n",
      "Processing frame 259\n",
      "Processing frame 260\n",
      "Processing frame 261\n",
      "Processing frame 262\n",
      "Processing frame 263\n",
      "Processing frame 264\n",
      "Processing frame 265\n",
      "Processing frame 266\n",
      "Processing frame 267\n",
      "Processing frame 268\n",
      "Processing frame 269\n",
      "Processing frame 270\n",
      "Processing frame 271\n",
      "Processing frame 272\n",
      "Processing frame 273\n",
      "Processing frame 274\n",
      "Processing frame 275\n",
      "Processing frame 276\n",
      "Processing frame 277\n",
      "Processing frame 278\n",
      "Processing frame 279\n",
      "Processing frame 280\n",
      "Processing frame 281\n",
      "Processing frame 282\n",
      "Processing frame 283\n",
      "Processing frame 284\n",
      "Processing frame 285\n",
      "Processing frame 286\n",
      "Processing frame 287\n",
      "Processing frame 288\n",
      "Processing frame 289\n",
      "Processing frame 290\n",
      "Processing frame 291\n",
      "Processing frame 292\n",
      "Processing frame 293\n",
      "Processing frame 294\n",
      "Processing frame 295\n",
      "Processing frame 296\n",
      "Processing frame 297\n",
      "Processing frame 298\n",
      "Processing frame 299\n",
      "Processing frame 300\n",
      "Processing frame 301\n",
      "Processing frame 302\n",
      "Processing frame 303\n",
      "Processing frame 304\n",
      "Processing frame 305\n",
      "Processing frame 306\n",
      "Processing frame 307\n",
      "Processing frame 308\n",
      "Processing frame 309\n",
      "Processing frame 310\n",
      "Processing frame 311\n",
      "Processing frame 312\n",
      "Processing frame 313\n",
      "Processing frame 314\n",
      "Processing frame 315\n",
      "Processing frame 316\n",
      "Processing frame 317\n",
      "Processing frame 318\n",
      "Processing frame 319\n",
      "Processing frame 320\n",
      "Processing frame 321\n",
      "Processing frame 322\n",
      "Processing frame 323\n",
      "Processing frame 324\n",
      "Processing frame 325\n",
      "Processing frame 326\n",
      "Processing frame 327\n",
      "Processing frame 328\n",
      "Processing frame 329\n",
      "Processing frame 330\n",
      "Processing frame 331\n",
      "Processing frame 332\n",
      "Processing frame 333\n",
      "Processing frame 334\n",
      "Processing frame 335\n",
      "Processing frame 336\n",
      "Processing frame 337\n",
      "Processing frame 338\n",
      "Processing frame 339\n",
      "Processing frame 340\n",
      "Processing frame 341\n",
      "Processing frame 342\n",
      "Processing frame 343\n",
      "Processing frame 344\n",
      "Processing frame 345\n",
      "Processing frame 346\n",
      "Processing frame 347\n",
      "Processing frame 348\n",
      "Processing frame 349\n",
      "Processing frame 350\n",
      "Processing frame 351\n",
      "Processing frame 352\n",
      "Processing frame 353\n",
      "Processing frame 354\n",
      "Processing frame 355\n",
      "Processing frame 356\n",
      "Processing frame 357\n",
      "Processing frame 358\n",
      "Processing frame 359\n",
      "Processing frame 360\n",
      "Processing frame 361\n",
      "Processing frame 362\n",
      "Processing frame 363\n",
      "Processing frame 364\n",
      "Processing frame 365\n",
      "Processing frame 366\n",
      "Processing frame 367\n",
      "Processing frame 368\n",
      "Processing frame 369\n",
      "Processing frame 370\n",
      "Processing frame 371\n",
      "Processing frame 372\n",
      "Processing frame 373\n",
      "Processing frame 374\n",
      "Processing frame 375\n",
      "Processing frame 376\n",
      "Processing frame 377\n",
      "Processing frame 378\n",
      "Processing frame 379\n",
      "Processing frame 380\n",
      "Processing frame 381\n",
      "Processing frame 382\n",
      "Processing frame 383\n",
      "Processing frame 384\n",
      "Processing frame 385\n",
      "Processing frame 386\n",
      "Processing frame 387\n",
      "Processing frame 388\n",
      "Processing frame 389\n",
      "Processing frame 390\n",
      "Processing frame 391\n",
      "Processing frame 392\n",
      "Processing frame 393\n",
      "Processing frame 394\n",
      "Processing frame 395\n",
      "Processing frame 396\n",
      "Processing frame 397\n",
      "Processing frame 398\n",
      "Processing frame 399\n",
      "Processing frame 400\n",
      "Processing frame 401\n",
      "Processing frame 402\n",
      "Processing frame 403\n",
      "Processing frame 404\n",
      "Processing frame 405\n",
      "Processing frame 406\n",
      "Processing frame 407\n",
      "Processing frame 408\n",
      "Processing frame 409\n",
      "Processing frame 410\n",
      "Processing frame 411\n",
      "Processing frame 412\n",
      "Processing frame 413\n",
      "Processing frame 414\n",
      "Processing frame 415\n",
      "Processing frame 416\n",
      "Processing frame 417\n",
      "Processing frame 418\n",
      "Processing frame 419\n",
      "Processing frame 420\n",
      "Processing frame 421\n",
      "Processing frame 422\n",
      "Processing frame 423\n",
      "Processing frame 424\n",
      "Processing frame 425\n",
      "Processing frame 426\n",
      "Processing frame 427\n",
      "Processing frame 428\n",
      "Processing frame 429\n",
      "Processing frame 430\n",
      "Processing frame 431\n",
      "Processing frame 432\n",
      "Processing frame 433\n",
      "Processing frame 434\n",
      "Processing frame 435\n",
      "Processing frame 436\n",
      "Processing frame 437\n",
      "Processing frame 438\n",
      "Processing frame 439\n",
      "Processing frame 440\n",
      "Processing frame 441\n",
      "Processing frame 442\n",
      "Processing frame 443\n",
      "Processing frame 444\n",
      "Processing frame 445\n",
      "Processing frame 446\n",
      "Processing frame 447\n",
      "Processing frame 448\n",
      "Processing frame 449\n",
      "Processing frame 450\n",
      "Processing frame 451\n",
      "Processing frame 452\n",
      "Processing frame 453\n",
      "Processing frame 454\n",
      "Processing frame 455\n",
      "Processing frame 456\n",
      "Processing frame 457\n",
      "Processing frame 458\n",
      "Processing frame 459\n",
      "Processing frame 460\n",
      "Processing frame 461\n",
      "Processing frame 462\n",
      "Processing frame 463\n",
      "Processing frame 464\n",
      "Processing frame 465\n",
      "Processing frame 466\n",
      "Processing frame 467\n",
      "Processing frame 468\n",
      "Processing frame 469\n",
      "Processing frame 470\n",
      "Processing frame 471\n",
      "Processing frame 472\n",
      "Processing frame 473\n",
      "Processing frame 474\n",
      "Processing frame 475\n",
      "Processing frame 476\n",
      "Processing frame 477\n",
      "Processing frame 478\n",
      "Processing frame 479\n",
      "Processing frame 480\n",
      "Processing frame 481\n",
      "Processing frame 482\n",
      "Processing frame 483\n",
      "Processing frame 484\n",
      "Processing frame 485\n",
      "Processing frame 486\n",
      "Processing frame 487\n",
      "Processing frame 488\n",
      "Processing frame 489\n",
      "Processing frame 490\n",
      "Processing frame 491\n",
      "Processing frame 492\n",
      "Processed 492 frames\n",
      "Average FPS: 65.58\n",
      "Output saved to: output\\video (49)_fall_detection.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from collections import deque\n",
    "from utils.datasets import letterbox\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from utils.general import non_max_suppression_kpt, strip_optimizer\n",
    "from torchvision import transforms\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, poseweights='yolov7-w6-pose.pt', device='0'):\n",
    "        \"\"\"\n",
    "        Initialize the Fall Detector with parameters as defined in the paper\n",
    "        \"Enhanced Fall Detection Using YOLOv7-W6-Pose for Real-Time Elderly Monitoring\"\n",
    "        \n",
    "        Key parameters:\n",
    "        - LENGTH_FACTOR_ALPHA (α): Used in height condition formula (Section 3.1)\n",
    "        - VELOCITY_THRESHOLD: Threshold for fall speed detection (Section 3.2)\n",
    "        - LEG_ANGLE_THRESHOLD: Degrees threshold for leg angles (Section 3.2)\n",
    "        - TORSO_ANGLE_THRESHOLD: Degrees threshold for torso orientation (Section 3.2)\n",
    "        - ASPECT_RATIO_THRESHOLD: Width/height ratio threshold (Section 3.1)\n",
    "        - CONFIDENCE_THRESHOLD: Minimum keypoint confidence for reliable detection\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Fall Detector with weights: {poseweights} on device: {device}\")\n",
    "        \n",
    "        # Select the appropriate device\n",
    "        self.device = select_device(device)\n",
    "        self.half = self.device.type != 'cpu'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = attempt_load(poseweights, map_location=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        \n",
    "        # Threshold parameters as defined in the paper\n",
    "        self.LENGTH_FACTOR_ALPHA = 0.5  # α in the height condition formula\n",
    "        self.VELOCITY_THRESHOLD = 1.0    # px/frame for fall speed\n",
    "        self.LEG_ANGLE_THRESHOLD = 45    # degrees for leg angles\n",
    "        self.TORSO_ANGLE_THRESHOLD = 50  # degrees for torso orientation\n",
    "        self.ASPECT_RATIO_THRESHOLD = 0.8 # width/height ratio\n",
    "        self.CONFIDENCE_THRESHOLD = 0.4  # minimum keypoint confidence\n",
    "        self.TARGET_FPS = 25\n",
    "        \n",
    "        # State tracking variables\n",
    "        self.prev_keypoints = None\n",
    "        self.velocity_buffer = deque(maxlen=3)  # tracks vertical speed\n",
    "        self.fall_buffer = deque(maxlen=2)      # confirmation buffer\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.prev_shoulder_y = None\n",
    "        \n",
    "        # Fall detection status\n",
    "        self.fall_detected = False\n",
    "    \n",
    "    def calculate_euclidean_distance(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between two points\n",
    "        Used in the paper to measure distances between key body points,\n",
    "        particularly for the Lfactor (length factor) calculation in Section 3.1\n",
    "        \n",
    "        Args:\n",
    "            point1, point2: Coordinate points (x,y)\n",
    "        Returns:\n",
    "            Euclidean distance between the points\n",
    "        \"\"\"\n",
    "        return math.hypot(point1[0]-point2[0], point1[1]-point2[1])\n",
    "\n",
    "    def calculate_angle(self, a, b, c):\n",
    "        \"\"\"\n",
    "        Calculate angle between three points (in degrees)\n",
    "        Used in the paper for calculating leg angles (Section 3.2)\n",
    "        \n",
    "        Args:\n",
    "            a, b, c: Three points where b is the vertex\n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ba = np.array([a[0]-b[0], a[1]-b[1]])\n",
    "            bc = np.array([c[0]-b[0], c[1]-b[1]])\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "            return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "        except:\n",
    "            return 180  # return maximum angle if calculation fails\n",
    "\n",
    "    def calculate_torso_angle(self, shoulders, hips):\n",
    "        \"\"\"\n",
    "        Calculate torso angle relative to vertical axis\n",
    "        Implements the torso orientation assessment described in Section 3.2\n",
    "        of the paper to detect when the torso is horizontal (fallen state)\n",
    "        \n",
    "        Args:\n",
    "            shoulders: list of shoulder points [(x,y), (x,y)]\n",
    "            hips: list of hip points [(x,y), (x,y)]\n",
    "        Returns:\n",
    "            angle in degrees between torso and vertical axis\n",
    "        \"\"\"\n",
    "        shoulder_center = np.mean(shoulders, axis=0)\n",
    "        hip_center = np.mean(hips, axis=0)\n",
    "        vertical_vector = np.array([0, 1])\n",
    "        torso_vector = np.array([hip_center[0]-shoulder_center[0], \n",
    "                                hip_center[1]-shoulder_center[1]])\n",
    "        \n",
    "        if np.linalg.norm(torso_vector) < 1e-6:\n",
    "            return 90  # neutral angle if points overlap\n",
    "            \n",
    "        cosine = np.dot(torso_vector, vertical_vector) / (np.linalg.norm(torso_vector) + 1e-6)\n",
    "        return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"\n",
    "        Main fall detection function implementing the paper's algorithm from Sections 3.1 and 3.2\n",
    "        Combines multiple conditions (height, velocity, angles, aspect ratio) to detect falls\n",
    "        \n",
    "        Args:\n",
    "            keypoints: Array of 17 keypoints with (x,y,confidence)\n",
    "        Returns:\n",
    "            tuple: (is_fall, state, condition_info)\n",
    "        \"\"\"\n",
    "        # Keypoint indices as defined in the paper\n",
    "        NOSE = 0\n",
    "        LEFT_SHOULDER = 5\n",
    "        RIGHT_SHOULDER = 6\n",
    "        LEFT_HIP = 11\n",
    "        RIGHT_HIP = 12\n",
    "        LEFT_KNEE = 13\n",
    "        RIGHT_KNEE = 14\n",
    "        LEFT_ANKLE = 15\n",
    "        RIGHT_ANKLE = 16\n",
    "        \n",
    "        try:\n",
    "            # Extract keypoints with confidence check\n",
    "            kp = {}\n",
    "            \n",
    "            # Reshape keypoints to get (x, y, conf) format for each keypoint\n",
    "            reshaped_kpts = keypoints.reshape(-1, 3)\n",
    "            \n",
    "            # Extract specific keypoints\n",
    "            kp['nose'] = reshaped_kpts[NOSE]\n",
    "            kp['left_shoulder'] = reshaped_kpts[LEFT_SHOULDER]\n",
    "            kp['right_shoulder'] = reshaped_kpts[RIGHT_SHOULDER]\n",
    "            kp['left_hip'] = reshaped_kpts[LEFT_HIP]\n",
    "            kp['right_hip'] = reshaped_kpts[RIGHT_HIP]\n",
    "            kp['left_knee'] = reshaped_kpts[LEFT_KNEE]\n",
    "            kp['right_knee'] = reshaped_kpts[RIGHT_KNEE]\n",
    "            kp['left_ankle'] = reshaped_kpts[LEFT_ANKLE]\n",
    "            kp['right_ankle'] = reshaped_kpts[RIGHT_ANKLE]\n",
    "            \n",
    "            # Confidence check for all keypoints\n",
    "            if any(point[2] < self.CONFIDENCE_THRESHOLD for point in kp.values()):\n",
    "                return False, \"low_confidence\", []\n",
    "\n",
    "            # Get coordinates (convert to tuples for clarity)\n",
    "            ls = (kp['left_shoulder'][0], kp['left_shoulder'][1])\n",
    "            rs = (kp['right_shoulder'][0], kp['right_shoulder'][1])\n",
    "            lh = (kp['left_hip'][0], kp['left_hip'][1])\n",
    "            rh = (kp['right_hip'][0], kp['right_hip'][1])\n",
    "            lk = (kp['left_knee'][0], kp['left_knee'][1])\n",
    "            rk = (kp['right_knee'][0], kp['right_knee'][1])\n",
    "            la = (kp['left_ankle'][0], kp['left_ankle'][1])\n",
    "            ra = (kp['right_ankle'][0], kp['right_ankle'][1])\n",
    "\n",
    "            \"\"\" 1. HEIGHT CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Calculate length factor (Lfactor) as Euclidean distance\n",
    "            torso_mid = ((lh[0] + rh[0])/2, (lh[1] + rh[1])/2)\n",
    "            Lfactor = self.calculate_euclidean_distance(ls, torso_mid)\n",
    "            \n",
    "            # Get vertical positions\n",
    "            max_feet_y = max(la[1], ra[1])\n",
    "            min_shoulder_y = min(ls[1], rs[1])\n",
    "            \n",
    "            # Paper's height condition: yl ≤ yFl + α·Lfactor\n",
    "            height_cond = min_shoulder_y >= (max_feet_y - self.LENGTH_FACTOR_ALPHA * Lfactor)\n",
    "            \n",
    "            \"\"\" 2. VELOCITY CONDITION (Paper Section 3.2) \"\"\"\n",
    "            current_time = time.time()\n",
    "            vertical_speed = 0\n",
    "            current_min_y = min(ls[1], rs[1])\n",
    "            \n",
    "            if self.prev_shoulder_y is not None and self.prev_frame_time is not None:\n",
    "                time_elapsed = current_time - self.prev_frame_time\n",
    "                if time_elapsed > 0:\n",
    "                    vertical_speed = (current_min_y - self.prev_shoulder_y) / time_elapsed\n",
    "                    self.velocity_buffer.append(abs(vertical_speed))\n",
    "            \n",
    "            avg_speed = sum(self.velocity_buffer)/len(self.velocity_buffer) if self.velocity_buffer else 0\n",
    "            speed_cond = avg_speed >= self.VELOCITY_THRESHOLD\n",
    "            \n",
    "            \"\"\" 3. ANGLE CONDITIONS (Paper Section 3.2) \"\"\"\n",
    "            left_leg_angle = self.calculate_angle(lh, lk, la)\n",
    "            right_leg_angle = self.calculate_angle(rh, rk, ra)\n",
    "            leg_angle_cond = min(left_leg_angle, right_leg_angle) < self.LEG_ANGLE_THRESHOLD\n",
    "            \n",
    "            # Torso orientation (not explicitly in paper but mentioned in text)\n",
    "            torso_angle = self.calculate_torso_angle([ls, rs], [lh, rh])\n",
    "            torso_cond = torso_angle > self.TORSO_ANGLE_THRESHOLD\n",
    "            \n",
    "            \"\"\" 4. ASPECT RATIO CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Body orientation ratio: width/height\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            head_to_feet = abs(kp['nose'][1] - max_feet_y)\n",
    "            orientation_ratio = body_width / (head_to_feet + 1e-6)\n",
    "            aspect_cond = orientation_ratio > self.ASPECT_RATIO_THRESHOLD\n",
    "            \n",
    "            \"\"\" FALL DECISION LOGIC (Paper Section 3) \"\"\"\n",
    "            # Combined conditions - at least 2 must be true\n",
    "            conditions_met = sum([height_cond, speed_cond, leg_angle_cond, torso_cond, aspect_cond])\n",
    "            \n",
    "            # State determination\n",
    "            current_state = \"normal\"\n",
    "            conditions_info = []\n",
    "            \n",
    "            if height_cond:\n",
    "                if speed_cond:  # Rapid descent\n",
    "                    current_state = \"falling\"\n",
    "                    self.fall_start_time = current_time\n",
    "                    conditions_info.append(f\"speed:{avg_speed:.1f}px/s\")\n",
    "                elif torso_cond and self.fall_start_time and (current_time - self.fall_start_time < 1.0):\n",
    "                    current_state = \"fallen\"\n",
    "                    conditions_info.append(\"horizontal\")\n",
    "            \n",
    "            if leg_angle_cond:\n",
    "                conditions_info.append(f\"leg_angle:{min(left_leg_angle, right_leg_angle):.0f}°\")\n",
    "            \n",
    "            if aspect_cond:\n",
    "                conditions_info.append(f\"aspect:{orientation_ratio:.2f}\")\n",
    "            \n",
    "            # Final decision with confirmation buffer\n",
    "            is_fall = conditions_met >= 2\n",
    "            self.fall_buffer.append(is_fall)\n",
    "            final_detection = sum(self.fall_buffer) >= 2 if len(self.fall_buffer) >= 1 else is_fall\n",
    "            \n",
    "            if final_detection:\n",
    "                current_state = \"fallen\"\n",
    "                self.fall_detected = True\n",
    "            else:\n",
    "                self.fall_detected = False\n",
    "            \n",
    "            # Update tracking variables\n",
    "            self.prev_keypoints = kp\n",
    "            self.prev_shoulder_y = current_min_y\n",
    "            self.prev_frame_time = current_time\n",
    "            \n",
    "            # Diagnostic information\n",
    "            conditions_info.extend([\n",
    "                f\"height:{'Y' if height_cond else 'N'}\",\n",
    "                f\"speed:{'Y' if speed_cond else 'N'}\",\n",
    "                f\"leg_angle:{'Y' if leg_angle_cond else 'N'}\",\n",
    "                f\"torso:{'Y' if torso_cond else 'N'}\",\n",
    "                f\"aspect:{'Y' if aspect_cond else 'N'}\",\n",
    "                f\"conf:{min(p[2] for p in kp.values()):.2f}\"\n",
    "            ])\n",
    "            \n",
    "            return final_detection, current_state, conditions_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Detection error: {str(e)}\")\n",
    "            return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for fall detection\n",
    "        \n",
    "        Args:\n",
    "            frame: Video frame to process\n",
    "            \n",
    "        Returns:\n",
    "            frame: Processed frame with detections\n",
    "            is_fall: Boolean indicating whether a fall was detected\n",
    "            state: Current state (normal, falling, fallen)\n",
    "            condition_info: List of conditions that triggered the detection\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        orig_image = frame.copy()\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image while maintaining aspect ratio\n",
    "        frame_height, frame_width = orig_image.shape[:2]\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        image = image.float()\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(image)\n",
    "            \n",
    "        # Post-process\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=self.model.yaml['nc'], nkpt=self.model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        \n",
    "        # Convert back to BGR for display\n",
    "        img = image[0].permute(1, 2, 0) * 255\n",
    "        img = img.cpu().numpy().astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize fall status and state for this frame\n",
    "        is_fall = False\n",
    "        current_state = \"normal\"\n",
    "        condition_info = []\n",
    "        \n",
    "        # Process each person detected\n",
    "        for idx in range(output.shape[0]):\n",
    "            # Draw skeleton and keypoints\n",
    "            plot_skeleton_kpts(img, output[idx, 7:].T, 3)\n",
    "            \n",
    "            # Calculate improved bounding box based on keypoints (YouTube approach)\n",
    "            # Find the minimum and maximum x,y coordinates from all keypoints\n",
    "            kpts = output[idx, 7:].reshape(-1, 3)\n",
    "            \n",
    "            # Initialize with first keypoint\n",
    "            x_values = [kpt[0] for kpt in kpts if kpt[2] > 0.5]  # Only use keypoints with confidence > 0.5\n",
    "            y_values = [kpt[1] for kpt in kpts if kpt[2] > 0.5]\n",
    "            \n",
    "            if x_values and y_values:  # Check if we have valid keypoints\n",
    "                xmin, ymin = min(x_values), min(y_values)\n",
    "                xmax, ymax = max(x_values), max(y_values)\n",
    "                \n",
    "                # Add padding to make bounding box a bit larger\n",
    "                padding = 10\n",
    "                xmin = max(0, xmin - padding)\n",
    "                ymin = max(0, ymin - padding)\n",
    "                xmax = xmax + padding\n",
    "                ymax = ymax + padding\n",
    "                \n",
    "                # Calculate aspect ratio for reference (not used in detection)\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                bbox_aspect_ratio = width / height if height > 0 else 0\n",
    "                \n",
    "                # Calculate center\n",
    "                cx = int((xmin + xmax) // 2)\n",
    "                cy = int((ymin + ymax) // 2)\n",
    "                \n",
    "                # For debugging: show aspect ratio on frame\n",
    "                cv2.putText(img, f\"Ratio: {bbox_aspect_ratio:.2f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                # Fallback to original bounding box if no valid keypoints\n",
    "                x1, y1, x2, y2 = output[idx, 0], output[idx, 1], output[idx, 2], output[idx, 3]\n",
    "                xmin, ymin = x1, y1\n",
    "                xmax, ymax = x2, y2\n",
    "                cx, cy = int((x1 + x2) // 2), int((y1 + y2) // 2)\n",
    "            \n",
    "            # Get key points for this person\n",
    "            key_points = output[idx, 7:]\n",
    "            \n",
    "            # Detect fall for this person using enhanced algorithm\n",
    "            person_fall, person_state, person_conditions = self.detect_fall(key_points)\n",
    "            \n",
    "            # If any person is falling, set global fall status\n",
    "            if person_fall:\n",
    "                is_fall = True\n",
    "                current_state = person_state\n",
    "                condition_info = person_conditions\n",
    "                \n",
    "                # Add visual indication of fall\n",
    "                status_text = f\"FALL DETECTED: {person_state.upper()}\"\n",
    "                cv2.putText(img, status_text, (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Draw the bounding box in red for a fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255), 2)\n",
    "                \n",
    "                # For YouTube-style visual, add a colored rectangle at the center\n",
    "                cv2.rectangle(img, (cx-10, cy-10), (cx+10, cy+10), (84, 61, 247), -1)\n",
    "                \n",
    "                # Add condition info to the frame\n",
    "                for i, cond in enumerate(person_conditions[:3]):  # Show first 3 conditions only\n",
    "                    cv2.putText(img, cond, (10, 60 + i*25), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)\n",
    "            else:\n",
    "                # Draw normal bounding box in green for no fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 1)\n",
    "                \n",
    "                # Show normal state\n",
    "                cv2.putText(img, f\"State: {person_state}\", (10, 60), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return img, is_fall, current_state, condition_info\n",
    "\n",
    "def run_fall_detection(poseweights='yolov7-w6-pose.pt', source='pose.mp4', device='cpu', display=True, save_output=True):\n",
    "    \"\"\"\n",
    "    Run fall detection on a video or webcam feed\n",
    "    \n",
    "    Args:\n",
    "        poseweights: Path to the YOLOv7 pose weights\n",
    "        source: Path to video file or webcam ID (0, 1, etc.)\n",
    "        device: Device to run inference on ('cpu' or '0', '1', etc. for GPU)\n",
    "        display: Whether to show video with detections in real-time\n",
    "        save_output: Whether to save the output video\n",
    "    \"\"\"\n",
    "    # Initialize the fall detector\n",
    "    detector = FallDetector(poseweights=poseweights, device=device)\n",
    "    \n",
    "    # Parse the input source\n",
    "    input_path = source\n",
    "    if source.isnumeric():\n",
    "        input_path = int(source)\n",
    "    \n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Setup output video writer if requested\n",
    "    out = None\n",
    "    if save_output:\n",
    "        if isinstance(input_path, int):\n",
    "            # For webcam\n",
    "            output_path = os.path.join('output', f\"webcam_fall_detection.mp4\")\n",
    "        else:\n",
    "            # For video file\n",
    "            filename = os.path.basename(input_path).split('.')[0]\n",
    "            output_path = os.path.join('output', f\"{filename}_fall_detection.mp4\")\n",
    "        \n",
    "        # Create VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    total_fps = 0\n",
    "    \n",
    "    print(\"Starting fall detection...\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}\")\n",
    "        \n",
    "        # Process frame for fall detection\n",
    "        start_time = time.time()\n",
    "        processed_frame, is_fall, current_state, condition_info = detector.process_frame(frame)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate FPS\n",
    "        processing_fps = 1 / (end_time - start_time)\n",
    "        total_fps += processing_fps\n",
    "        \n",
    "        # Resize processed frame to match original dimensions for display and saving\n",
    "        processed_frame_resized = cv2.resize(processed_frame, (frame_width, frame_height))\n",
    "        \n",
    "        # Add FPS info\n",
    "        cv2.putText(processed_frame_resized, f\"FPS: {processing_fps:.2f}\", (frame_width - 150, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame if requested\n",
    "        if display:\n",
    "            cv2.imshow('Fall Detection', processed_frame_resized)\n",
    "            \n",
    "            # Exit on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Save frame to output video if requested\n",
    "        if save_output and out is not None:\n",
    "            out.write(processed_frame_resized)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if save_output and out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print statistics\n",
    "    if frame_count > 0:\n",
    "        avg_fps = total_fps / frame_count\n",
    "        print(f\"Processed {frame_count} frames\")\n",
    "        print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "        if save_output:\n",
    "            print(f\"Output saved to: {output_path}\")\n",
    "\n",
    "def run_interactive():\n",
    "    \"\"\"\n",
    "    Interactive function to run fall detection with user input\n",
    "    \"\"\"\n",
    "    # Get the weights file\n",
    "    poseweights = input(\"Enter path to weights file [default: yolov7-w6-pose.pt]: \") or \"yolov7-w6-pose.pt\"\n",
    "    \n",
    "    # Get device type\n",
    "    use_gpu = input(\"Use GPU? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    if use_gpu == \"y\":\n",
    "        device = input(\"Enter GPU device ID [default: 0]: \") or \"0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Get source type\n",
    "    print(\"\\nSelect input source:\")\n",
    "    print(\"1: Video file\")\n",
    "    print(\"2: Webcam\")\n",
    "    source_choice = input(\"Enter choice [1/2]: \")\n",
    "    \n",
    "    if source_choice == \"1\":\n",
    "        # Video file\n",
    "        default_video = \"sample_video.mp4\"\n",
    "        source = input(f\"Enter video file path [default: {default_video}]: \") or default_video\n",
    "        # Ask if user wants to display the processed video in real-time\n",
    "        display_video = input(\"Display video with pose estimation in real-time? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    else:\n",
    "        # Webcam\n",
    "        cam_id = input(\"Enter webcam ID [default: 0]: \") or \"0\"\n",
    "        source = cam_id\n",
    "        display_video = \"y\"  # Always display for webcam\n",
    "    \n",
    "    # Ask if user wants to save the output video\n",
    "    save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "    print(f\"\\nRunning fall detection with:\")\n",
    "    print(f\"- Weights: {poseweights}\")\n",
    "    print(f\"- Device: {device}\")\n",
    "    print(f\"- Source: {source}\")\n",
    "    print(f\"- Display: {'Yes' if display_video == 'y' else 'No'}\")\n",
    "    print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "    confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    \n",
    "    if confirmation == \"y\":\n",
    "        # Run the model\n",
    "        run_with_display = (display_video == \"y\")\n",
    "        save_output = (save_video == \"y\")\n",
    "        \n",
    "        # First strip optimizer to ensure model works correctly\n",
    "        strip_optimizer(device, poseweights)\n",
    "        \n",
    "        # Run fall detection\n",
    "        run_fall_detection(\n",
    "            poseweights=poseweights,\n",
    "            source=source,\n",
    "            device=device,\n",
    "            display=run_with_display,\n",
    "            save_output=save_output\n",
    "        )\n",
    "    else:\n",
    "        print(\"Operation cancelled\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run interactively\n",
    "    run_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d53466-363a-416c-a710-3d71accbc71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
