{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5082daf8-13cb-441a-9f40-083cc4d4195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.datasets import letterbox\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from utils.general import non_max_suppression_kpt, strip_optimizer\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4cd468c-5177-4f87-8446-db7a5096b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--weights WEIGHTS] --source SOURCE [--device DEVICE] [--no-display] [--no-save]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --source\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naufa\\anaconda3\\envs\\cv_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class FallDetector:\n",
    "    def __init__(self, poseweights='yolov7-w6-pose.pt', device='0'):\n",
    "        \"\"\"\n",
    "        Initialize the Fall Detector\n",
    "        \n",
    "        Args:\n",
    "            poseweights: Path to the YOLOv7 pose weights\n",
    "            device: Device to run inference on ('cpu' or '0', '1', etc. for GPU)\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Fall Detector with weights: {poseweights} on device: {device}\")\n",
    "        \n",
    "        # Select the appropriate device\n",
    "        self.device = select_device(device)\n",
    "        self.half = self.device.type != 'cpu'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = attempt_load(poseweights, map_location=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        \n",
    "        # Set thresholds for fall detection\n",
    "        self.fall_detected = False\n",
    "        self.prev_key_points = None\n",
    "        self.fall_count = 0  # Counter for consecutive fall detections\n",
    "        self.non_fall_count = 0  # Counter for consecutive non-fall detections\n",
    "        self.fall_threshold = 3  # Number of consecutive detections required to trigger fall alert\n",
    "        self.fall_history = []  # To store history of fall detections\n",
    "        \n",
    "        # Time tracking\n",
    "        self.last_fall_time = 0\n",
    "        \n",
    "    def detect_fall(self, key_points):\n",
    "        \"\"\"\n",
    "        Detect falls using the algorithm described in the journal\n",
    "        \n",
    "        Args:\n",
    "            key_points: Array of key points from YOLOv7-W6-Pose model\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if fall detected, False otherwise\n",
    "        \"\"\"\n",
    "        # Extract coordinates of key points as described in journal\n",
    "        # The key points are organized as [x1, y1, conf1, x2, y2, conf2, ...]\n",
    "        # We need to reorganize them to get the specific joints\n",
    "        \n",
    "        # Note: COCO keypoint format:\n",
    "        # 0: nose, 1: left_eye, 2: right_eye, 3: left_ear, 4: right_ear, \n",
    "        # 5: left_shoulder, 6: right_shoulder, 7: left_elbow, 8: right_elbow,\n",
    "        # 9: left_wrist, 10: right_wrist, 11: left_hip, 12: right_hip,\n",
    "        # 13: left_knee, 14: right_knee, 15: left_ankle, 16: right_ankle\n",
    "        \n",
    "        # If we don't have all key points needed for fall detection, return False\n",
    "        if key_points.shape[0] < 17:\n",
    "            return False\n",
    "            \n",
    "        # Extract key points as per journal's algorithm\n",
    "        # Left and right shoulders\n",
    "        left_shoulder = key_points[5]\n",
    "        right_shoulder = key_points[6]\n",
    "        \n",
    "        # Torso (left and right hip)\n",
    "        left_torso = key_points[11]  # Left hip\n",
    "        right_torso = key_points[12]  # Right hip\n",
    "        \n",
    "        # Feet (left and right ankles)\n",
    "        left_foot = key_points[15]  # Left ankle\n",
    "        right_foot = key_points[16]  # Right ankle\n",
    "        \n",
    "        # Extract coordinates\n",
    "        xl, yl = left_shoulder[0], left_shoulder[1]  # Left shoulder coordinates\n",
    "        xr, yr = right_shoulder[0], right_shoulder[1]  # Right shoulder coordinates\n",
    "        xTl, yTl = left_torso[0], left_torso[1]  # Left hip coordinates\n",
    "        xTr, yTr = right_torso[0], right_torso[1]  # Right hip coordinates\n",
    "        xFl, yFl = left_foot[0], left_foot[1]  # Left ankle coordinates\n",
    "        xFr, yFr = right_foot[0], right_foot[1]  # Right ankle coordinates\n",
    "        \n",
    "        # Calculate length factor as described in the journal formula (1)\n",
    "        # Equation 1: Lfactor = sqrt((xl - xTl)^2 + (yl - yTl)^2)\n",
    "        length_factor = np.sqrt((xl - xTl) ** 2 + (yl - yTl) ** 2)\n",
    "        \n",
    "        # Check fall conditions as per journal's criteria\n",
    "        \n",
    "        # Condition 1: Check if shoulders are lower than feet (adjusted by length factor)\n",
    "        # Equation 2: yl ≤ yFl + α·Lfactor\n",
    "        alpha = 0.1  # Small adjustment factor\n",
    "        shoulder_below_feet = yl <= yFl + alpha * length_factor\n",
    "        \n",
    "        # Calculate body dimensions\n",
    "        # Equation 3: Hbody = |yl - yFl|\n",
    "        body_height = abs(yl - yFl)\n",
    "        \n",
    "        # Equation 4: Wbody = |xl - xr|\n",
    "        body_width = abs(xl - xr)\n",
    "        \n",
    "        # Condition 2: Check if body is \"stretched\" horizontally\n",
    "        # Equation 5: Hbody < Wbody\n",
    "        horizontally_stretched = body_height < body_width\n",
    "        \n",
    "        # Calculate vertical speed if we have previous key points\n",
    "        speed_threshold = 15.0  # Threshold for vertical speed\n",
    "        rapid_movement = False\n",
    "        \n",
    "        if self.prev_key_points is not None:\n",
    "            prev_yl = self.prev_key_points[5][1]  # Previous y-coordinate of left shoulder\n",
    "            vertical_displacement = abs(yl - prev_yl)\n",
    "            # Calculate vertical speed (displacement per frame)\n",
    "            vertical_speed = vertical_displacement\n",
    "            rapid_movement = vertical_speed > speed_threshold\n",
    "        \n",
    "        # Store current key points for next frame comparison\n",
    "        self.prev_key_points = key_points\n",
    "        \n",
    "        # Calculate angle between torso and legs\n",
    "        # Using the angle between the line connecting hip to shoulder and the line connecting hip to ankle\n",
    "        angle_threshold = 45  # degrees\n",
    "        \n",
    "        # Calculate vectors\n",
    "        torso_vector = [xl - xTl, yl - yTl]\n",
    "        leg_vector = [xFl - xTl, yFl - yTl]\n",
    "        \n",
    "        # Calculate magnitude of vectors\n",
    "        torso_magnitude = np.sqrt(torso_vector[0]**2 + torso_vector[1]**2)\n",
    "        leg_magnitude = np.sqrt(leg_vector[0]**2 + leg_vector[1]**2)\n",
    "        \n",
    "        # Calculate the dot product\n",
    "        dot_product = torso_vector[0] * leg_vector[0] + torso_vector[1] * leg_vector[1]\n",
    "        \n",
    "        # Calculate the angle in degrees\n",
    "        if torso_magnitude * leg_magnitude != 0:  # Avoid division by zero\n",
    "            angle = np.arccos(dot_product / (torso_magnitude * leg_magnitude)) * 180 / np.pi\n",
    "            acute_angle = abs(angle < angle_threshold)\n",
    "        else:\n",
    "            acute_angle = False\n",
    "            \n",
    "        # Determine if a fall has occurred based on all conditions\n",
    "        # A fall is detected if:\n",
    "        # 1. Shoulders are below feet (adjusted by length factor), or\n",
    "        # 2. Body is more horizontal than vertical, and\n",
    "        # 3. There's a rapid vertical movement, or\n",
    "        # 4. The angle between torso and legs is acute\n",
    "        \n",
    "        is_fall = (shoulder_below_feet or horizontally_stretched) and (rapid_movement or acute_angle)\n",
    "        \n",
    "        return is_fall\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for fall detection\n",
    "        \n",
    "        Args:\n",
    "            frame: Video frame to process\n",
    "            \n",
    "        Returns:\n",
    "            frame: Processed frame with detections\n",
    "            is_fall: Boolean indicating whether a fall was detected\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        orig_image = frame.copy()\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image while maintaining aspect ratio\n",
    "        frame_height, frame_width = orig_image.shape[:2]\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        image = image.float()\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(image)\n",
    "            \n",
    "        # Post-process\n",
    "        output = non_max_suppression_kpt(output, 0.5, 0.65, nc=self.model.yaml['nc'], nkpt=self.model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        \n",
    "        # Convert back to BGR for display\n",
    "        img = image[0].permute(1, 2, 0) * 255\n",
    "        img = img.cpu().numpy().astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize fall status for this frame\n",
    "        is_fall = False\n",
    "        \n",
    "        # Process each person detected\n",
    "        for idx in range(output.shape[0]):\n",
    "            # Draw skeleton\n",
    "            plot_skeleton_kpts(img, output[idx, 7:].T, 3)\n",
    "            \n",
    "            # Get key points for this person\n",
    "            key_points = output[idx, 7:].reshape(-1, 3)  # reshape to (17, 3) [x, y, conf]\n",
    "            \n",
    "            # Detect fall for this person\n",
    "            person_fall = self.detect_fall(key_points)\n",
    "            \n",
    "            # If any person is falling, set is_fall to True\n",
    "            if person_fall:\n",
    "                is_fall = True\n",
    "                \n",
    "                # Add visual indication of fall\n",
    "                cv2.putText(img, \"FALL DETECTED\", (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Draw red bounding box around the person\n",
    "                x1, y1, x2, y2 = output[idx, 0], output[idx, 1], output[idx, 2], output[idx, 3]\n",
    "                cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        \n",
    "        # Update fall state based on consecutive detections\n",
    "        self.update_fall_state(is_fall)\n",
    "        \n",
    "        return img, self.fall_detected\n",
    "\n",
    "    def update_fall_state(self, is_fall):\n",
    "        \"\"\"\n",
    "        Update the fall state based on consecutive detections\n",
    "        \n",
    "        Args:\n",
    "            is_fall: Boolean indicating whether a fall was detected in current frame\n",
    "        \"\"\"\n",
    "        if is_fall:\n",
    "            self.fall_count += 1\n",
    "            self.non_fall_count = 0\n",
    "            \n",
    "            # If we have enough consecutive fall detections, set fall_detected to True\n",
    "            if self.fall_count >= self.fall_threshold:\n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Check if 2 minutes have passed since last fall alert\n",
    "                if current_time - self.last_fall_time > 120:  # 120 seconds = 2 minutes\n",
    "                    self.fall_detected = True\n",
    "                    self.last_fall_time = current_time\n",
    "        else:\n",
    "            self.fall_count = 0\n",
    "            self.non_fall_count += 1\n",
    "            \n",
    "            # If we have enough consecutive non-fall detections, reset fall_detected\n",
    "            if self.non_fall_count >= self.fall_threshold:\n",
    "                self.fall_detected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb23073-c4c6-4c8c-b7d1-ac530afedeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_fall_detection(poseweights='yolov7-w6-pose.pt', source='pose.mp4', device='cpu', display=True, save_output=True):\n",
    "    \"\"\"\n",
    "    Run fall detection on a video or webcam feed\n",
    "    \n",
    "    Args:\n",
    "        poseweights: Path to the YOLOv7 pose weights\n",
    "        source: Path to video file or webcam ID (0, 1, etc.)\n",
    "        device: Device to run inference on ('cpu' or '0', '1', etc. for GPU)\n",
    "        display: Whether to show video with detections in real-time\n",
    "        save_output: Whether to save the output video\n",
    "    \"\"\"\n",
    "    # Initialize the fall detector\n",
    "    detector = FallDetector(poseweights=poseweights, device=device)\n",
    "    \n",
    "    # Parse the input source\n",
    "    input_path = source\n",
    "    if source.isnumeric():\n",
    "        input_path = int(source)\n",
    "    \n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Setup output video writer if requested\n",
    "    out = None\n",
    "    if save_output:\n",
    "        if isinstance(input_path, int):\n",
    "            # For webcam\n",
    "            output_path = os.path.join('output', f\"webcam_fall_detection.mp4\")\n",
    "        else:\n",
    "            # For video file\n",
    "            filename = os.path.basename(input_path).split('.')[0]\n",
    "            output_path = os.path.join('output', f\"{filename}_fall_detection.mp4\")\n",
    "        \n",
    "        # Create VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    total_fps = 0\n",
    "    \n",
    "    print(\"Starting fall detection...\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}\")\n",
    "        \n",
    "        # Process frame for fall detection\n",
    "        start_time = time.time()\n",
    "        processed_frame, is_fall = detector.process_frame(frame)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate FPS\n",
    "        processing_fps = 1 / (end_time - start_time)\n",
    "        total_fps += processing_fps\n",
    "        \n",
    "        # Resize processed frame to match original dimensions for display and saving\n",
    "        processed_frame_resized = cv2.resize(processed_frame, (frame_width, frame_height))\n",
    "        \n",
    "        # Display fall status on frame\n",
    "        if is_fall:\n",
    "            # Red text for fall alert\n",
    "            cv2.putText(processed_frame_resized, \"FALL DETECTED!\", (50, 100), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "        \n",
    "        # Add FPS info\n",
    "        cv2.putText(processed_frame_resized, f\"FPS: {processing_fps:.2f}\", (50, 150), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame if requested\n",
    "        if display:\n",
    "            cv2.imshow('Fall Detection', processed_frame_resized)\n",
    "            \n",
    "            # Exit on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Save frame to output video if requested\n",
    "        if save_output and out is not None:\n",
    "            out.write(processed_frame_resized)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if save_output and out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print statistics\n",
    "    if frame_count > 0:\n",
    "        avg_fps = total_fps / frame_count\n",
    "        print(f\"Processed {frame_count} frames\")\n",
    "        print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "        if save_output:\n",
    "            print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ec914-4b07-408a-83c7-d51466acd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive():\n",
    "    \"\"\"\n",
    "    Interactive function to run fall detection with user input\n",
    "    \"\"\"\n",
    "    # Get the weights file\n",
    "    poseweights = input(\"Enter path to weights file [default: yolov7-w6-pose.pt]: \") or \"yolov7-w6-pose.pt\"\n",
    "    \n",
    "    # Get device type\n",
    "    use_gpu = input(\"Use GPU? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    if use_gpu == \"y\":\n",
    "        device = input(\"Enter GPU device ID [default: 0]: \") or \"0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Get source type\n",
    "    print(\"\\nSelect input source:\")\n",
    "    print(\"1: Video file\")\n",
    "    print(\"2: Webcam\")\n",
    "    source_choice = input(\"Enter choice [1/2]: \")\n",
    "    \n",
    "    if source_choice == \"1\":\n",
    "        # Video file\n",
    "        default_video = \"sample_video.mp4\"\n",
    "        source = input(f\"Enter video file path [default: {default_video}]: \") or default_video\n",
    "        # Ask if user wants to display the processed video in real-time\n",
    "        display_video = input(\"Display video with pose estimation in real-time? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    else:\n",
    "        # Webcam\n",
    "        cam_id = input(\"Enter webcam ID [default: 0]: \") or \"0\"\n",
    "        source = cam_id\n",
    "        display_video = \"y\"  # Always display for webcam\n",
    "    \n",
    "    # Ask if user wants to save the output video\n",
    "    save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "    print(f\"\\nRunning fall detection with:\")\n",
    "    print(f\"- Weights: {poseweights}\")\n",
    "    print(f\"- Device: {device}\")\n",
    "    print(f\"- Source: {source}\")\n",
    "    print(f\"- Display: {'Yes' if display_video == 'y' else 'No'}\")\n",
    "    print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "    confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    \n",
    "    if confirmation == \"y\":\n",
    "        # Run the model\n",
    "        run_with_display = (display_video == \"y\")\n",
    "        save_output = (save_video == \"y\")\n",
    "        \n",
    "        # First strip optimizer to ensure model works correctly\n",
    "        strip_optimizer(device, poseweights)\n",
    "        \n",
    "        # Run fall detection\n",
    "        run_fall_detection(\n",
    "            poseweights=poseweights,\n",
    "            source=source,\n",
    "            device=device,\n",
    "            display=run_with_display,\n",
    "            save_output=save_output\n",
    "        )\n",
    "    else:\n",
    "        print(\"Operation cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d03e37-3824-4c48-8bd3-b0217a1bf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run interactively\n",
    "run_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9813b-b672-4e3c-9013-ab5d68a217c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can run directly with specific parameters\n",
    "# Uncomment and modify the line below to run with specific parameters\n",
    "\n",
    "# run_fall_detection(poseweights='yolov7-w6-pose.pt', source='sample_video.mp4', device='0', display=True, save_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72d99e-7f50-48c2-86ee-b21aa3f4c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a saved video result (if available)\n",
    "def play_video_result(video_path):\n",
    "    \"\"\"Play a video file in the notebook\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file {video_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Playing video: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_delay = int(1000 / fps)  # Delay between frames in ms\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('Video Result', frame)\n",
    "        \n",
    "        # Break on 'q' press\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e05850-f424-41c9-822d-c6d2d8bbff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(metrics_file=None):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics from the fall detection system\n",
    "    \n",
    "    Args:\n",
    "        metrics_file: Optional path to a file containing performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Fall Detection System Performance Analysis\")\n",
    "    print(\"==========================================\")\n",
    "    \n",
    "    # Sample metrics (replace with actual metrics from your tests)\n",
    "    # These metrics represent the values from the paper\n",
    "    accuracy = 95.7\n",
    "    precision = 94.7\n",
    "    recall = 96.4\n",
    "    specificity = 95.0\n",
    "    f1_score = 95.5\n",
    "    \n",
    "    print(f\"Accuracy:    {accuracy}%\")\n",
    "    print(f\"Precision:   {precision}%\")\n",
    "    print(f\"Recall:      {recall}%\")\n",
    "    print(f\"Specificity: {specificity}%\")\n",
    "    print(f\"F1 Score:    {f1_score}\")\n",
    "    \n",
    "    # Comparison with other methods (as per the paper)\n",
    "    print(\"\\nComparison with other methods:\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Model          | Accuracy | Precision | Recall  | Specificity | F1 Score\")\n",
    "    print(\"---------------|----------|-----------|---------|-------------|--------\")\n",
    "    print(\"Chamle et al.  | 79.30%   | 84.30%    | 73.07%  | 79.40%      | 78.28%\")\n",
    "    print(\"Alaoui et al.  | 90.90%   | 94.56%    | 81.08%  | 90.84%      | 87.30%\")\n",
    "    print(\"Poonsri et al. | 86.21%   | 89.1%     | 93.18%  | 64.29%      | 91.1%\")\n",
    "    print(\"Our method     | 96.15%   | 97%       | 97.98%  | 90.32%      | 97.48%\")\n",
    "    \n",
    "    # Plot metrics for visualization (if matplotlib is available)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        methods = ['Chamle et al.', 'Alaoui et al.', 'Poonsri et al.', 'Our method']\n",
    "        accuracy_values = [79.30, 90.90, 86.21, 96.15]\n",
    "        precision_values = [84.30, 94.56, 89.1, 97.0]\n",
    "        recall_values = [73.07, 81.08, 93.18, 97.98]\n",
    "        specificity_values = [79.40, 90.84, 64.29, 90.32]\n",
    "        f1_values = [78.28, 87.30, 91.1, 97.48]\n",
    "        \n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1 Score']\n",
    "        \n",
    "        # Bar chart for all metrics\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        x = np.arange(len(methods))\n",
    "        width = 0.15\n",
    "        \n",
    "        plt.bar(x - 2*width, accuracy_values, width, label='Accuracy')\n",
    "        plt.bar(x - width, precision_values, width, label='Precision')\n",
    "        plt.bar(x, recall_values, width, label='Recall')\n",
    "        plt.bar(x + width, specificity_values, width, label='Specificity')\n",
    "        plt.bar(x + 2*width, f1_values, width, label='F1 Score')\n",
    "        \n",
    "        plt.ylabel('Percentage (%)')\n",
    "        plt.title('Performance Comparison of Fall Detection Methods')\n",
    "        plt.xticks(x, methods)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels on top of each bar\n",
    "        for i in range(len(methods)):\n",
    "            plt.text(i - 2*width, accuracy_values[i] + 1, f\"{accuracy_values[i]:.1f}\", ha='center', va='bottom', fontsize=8)\n",
    "            plt.text(i - width, precision_values[i] + 1, f\"{precision_values[i]:.1f}\", ha='center', va='bottom', fontsize=8)\n",
    "            plt.text(i, recall_values[i] + 1, f\"{recall_values[i]:.1f}\", ha='center', va='bottom', fontsize=8)\n",
    "            plt.text(i + width, specificity_values[i] + 1, f\"{specificity_values[i]:.1f}\", ha='center', va='bottom', fontsize=8)\n",
    "            plt.text(i + 2*width, f1_values[i] + 1, f\"{f1_values[i]:.1f}\", ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\nNote: Install matplotlib to visualize performance metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
