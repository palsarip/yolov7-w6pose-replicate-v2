{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d4688-5251-4619-b52d-f8c814c3dbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd506d5-8a9c-4703-8729-4b5e8d8bfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from collections import deque\n",
    "from utils.datasets import letterbox\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from utils.general import non_max_suppression_kpt, strip_optimizer\n",
    "from torchvision import transforms\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, poseweights='yolov7-w6-pose.pt', device='0'):\n",
    "        \"\"\"\n",
    "        Initialize the Fall Detector with parameters as defined in the paper\n",
    "        \"Enhanced Fall Detection Using YOLOv7-W6-Pose for Real-Time Elderly Monitoring\"\n",
    "        \n",
    "        Key parameters:\n",
    "        - LENGTH_FACTOR_ALPHA (α): Used in height condition formula (Section 3.1)\n",
    "        - VELOCITY_THRESHOLD: Threshold for fall speed detection (Section 3.2)\n",
    "        - LEG_ANGLE_THRESHOLD: Degrees threshold for leg angles (Section 3.2)\n",
    "        - TORSO_ANGLE_THRESHOLD: Degrees threshold for torso orientation (Section 3.2)\n",
    "        - ASPECT_RATIO_THRESHOLD: Width/height ratio threshold (Section 3.1)\n",
    "        - CONFIDENCE_THRESHOLD: Minimum keypoint confidence for reliable detection\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Fall Detector with weights: {poseweights} on device: {device}\")\n",
    "        \n",
    "        # Select the appropriate device\n",
    "        self.device = select_device(device)\n",
    "        self.half = self.device.type != 'cpu'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = attempt_load(poseweights, map_location=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        \n",
    "        # Threshold parameters as defined in the paper\n",
    "        self.LENGTH_FACTOR_ALPHA = 0.5  # α in the height condition formula\n",
    "        self.VELOCITY_THRESHOLD = 1.0    # px/frame for fall speed\n",
    "        self.LEG_ANGLE_THRESHOLD = 45    # degrees for leg angles\n",
    "        self.TORSO_ANGLE_THRESHOLD = 50  # degrees for torso orientation\n",
    "        self.ASPECT_RATIO_THRESHOLD = 0.8 # width/height ratio\n",
    "        self.CONFIDENCE_THRESHOLD = 0.4  # minimum keypoint confidence\n",
    "        self.TARGET_FPS = 25\n",
    "        \n",
    "        # State tracking variables\n",
    "        self.prev_keypoints = None\n",
    "        self.velocity_buffer = deque(maxlen=3)  # tracks vertical speed\n",
    "        self.fall_buffer = deque(maxlen=2)      # confirmation buffer\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.prev_shoulder_y = None\n",
    "        \n",
    "        # Fall detection status\n",
    "        self.fall_detected = False\n",
    "    \n",
    "    def calculate_euclidean_distance(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between two points\n",
    "        Used in the paper to measure distances between key body points,\n",
    "        particularly for the Lfactor (length factor) calculation in Section 3.1\n",
    "        \n",
    "        Args:\n",
    "            point1, point2: Coordinate points (x,y)\n",
    "        Returns:\n",
    "            Euclidean distance between the points\n",
    "        \"\"\"\n",
    "        return math.hypot(point1[0]-point2[0], point1[1]-point2[1])\n",
    "\n",
    "    def calculate_angle(self, a, b, c):\n",
    "        \"\"\"\n",
    "        Calculate angle between three points (in degrees)\n",
    "        Used in the paper for calculating leg angles (Section 3.2)\n",
    "        \n",
    "        Args:\n",
    "            a, b, c: Three points where b is the vertex\n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ba = np.array([a[0]-b[0], a[1]-b[1]])\n",
    "            bc = np.array([c[0]-b[0], c[1]-b[1]])\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "            return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "        except:\n",
    "            return 180  # return maximum angle if calculation fails\n",
    "\n",
    "    def calculate_torso_angle(self, shoulders, hips):\n",
    "        \"\"\"\n",
    "        Calculate torso angle relative to vertical axis\n",
    "        Implements the torso orientation assessment described in Section 3.2\n",
    "        of the paper to detect when the torso is horizontal (fallen state)\n",
    "        \n",
    "        Args:\n",
    "            shoulders: list of shoulder points [(x,y), (x,y)]\n",
    "            hips: list of hip points [(x,y), (x,y)]\n",
    "        Returns:\n",
    "            angle in degrees between torso and vertical axis\n",
    "        \"\"\"\n",
    "        shoulder_center = np.mean(shoulders, axis=0)\n",
    "        hip_center = np.mean(hips, axis=0)\n",
    "        vertical_vector = np.array([0, 1])\n",
    "        torso_vector = np.array([hip_center[0]-shoulder_center[0], \n",
    "                                hip_center[1]-shoulder_center[1]])\n",
    "        \n",
    "        if np.linalg.norm(torso_vector) < 1e-6:\n",
    "            return 90  # neutral angle if points overlap\n",
    "            \n",
    "        cosine = np.dot(torso_vector, vertical_vector) / (np.linalg.norm(torso_vector) + 1e-6)\n",
    "        return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"\n",
    "        Main fall detection function implementing the paper's algorithm from Sections 3.1 and 3.2\n",
    "        Combines multiple conditions (height, velocity, angles, aspect ratio) to detect falls\n",
    "        \n",
    "        Args:\n",
    "            keypoints: Array of 17 keypoints with (x,y,confidence)\n",
    "        Returns:\n",
    "            tuple: (is_fall, state, condition_info)\n",
    "        \"\"\"\n",
    "        # Keypoint indices as defined in the paper\n",
    "        NOSE = 0\n",
    "        LEFT_SHOULDER = 5\n",
    "        RIGHT_SHOULDER = 6\n",
    "        LEFT_HIP = 11\n",
    "        RIGHT_HIP = 12\n",
    "        LEFT_KNEE = 13\n",
    "        RIGHT_KNEE = 14\n",
    "        LEFT_ANKLE = 15\n",
    "        RIGHT_ANKLE = 16\n",
    "        \n",
    "        try:\n",
    "            # Extract keypoints with confidence check\n",
    "            kp = {}\n",
    "            \n",
    "            # Reshape keypoints to get (x, y, conf) format for each keypoint\n",
    "            reshaped_kpts = keypoints.reshape(-1, 3)\n",
    "            \n",
    "            # Extract specific keypoints\n",
    "            kp['nose'] = reshaped_kpts[NOSE]\n",
    "            kp['left_shoulder'] = reshaped_kpts[LEFT_SHOULDER]\n",
    "            kp['right_shoulder'] = reshaped_kpts[RIGHT_SHOULDER]\n",
    "            kp['left_hip'] = reshaped_kpts[LEFT_HIP]\n",
    "            kp['right_hip'] = reshaped_kpts[RIGHT_HIP]\n",
    "            kp['left_knee'] = reshaped_kpts[LEFT_KNEE]\n",
    "            kp['right_knee'] = reshaped_kpts[RIGHT_KNEE]\n",
    "            kp['left_ankle'] = reshaped_kpts[LEFT_ANKLE]\n",
    "            kp['right_ankle'] = reshaped_kpts[RIGHT_ANKLE]\n",
    "            \n",
    "            # Confidence check for all keypoints\n",
    "            if any(point[2] < self.CONFIDENCE_THRESHOLD for point in kp.values()):\n",
    "                return False, \"low_confidence\", []\n",
    "\n",
    "            # Get coordinates (convert to tuples for clarity)\n",
    "            ls = (kp['left_shoulder'][0], kp['left_shoulder'][1])\n",
    "            rs = (kp['right_shoulder'][0], kp['right_shoulder'][1])\n",
    "            lh = (kp['left_hip'][0], kp['left_hip'][1])\n",
    "            rh = (kp['right_hip'][0], kp['right_hip'][1])\n",
    "            lk = (kp['left_knee'][0], kp['left_knee'][1])\n",
    "            rk = (kp['right_knee'][0], kp['right_knee'][1])\n",
    "            la = (kp['left_ankle'][0], kp['left_ankle'][1])\n",
    "            ra = (kp['right_ankle'][0], kp['right_ankle'][1])\n",
    "\n",
    "            \"\"\" 1. HEIGHT CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Calculate length factor (Lfactor) as Euclidean distance\n",
    "            torso_mid = ((lh[0] + rh[0])/2, (lh[1] + rh[1])/2)\n",
    "            Lfactor = self.calculate_euclidean_distance(ls, torso_mid)\n",
    "            \n",
    "            # Get vertical positions\n",
    "            max_feet_y = max(la[1], ra[1])\n",
    "            min_shoulder_y = min(ls[1], rs[1])\n",
    "            \n",
    "            # Paper's height condition: yl ≤ yFl + α·Lfactor\n",
    "            height_cond = min_shoulder_y >= (max_feet_y - self.LENGTH_FACTOR_ALPHA * Lfactor)\n",
    "            \n",
    "            \"\"\" 2. VELOCITY CONDITION (Paper Section 3.2) \"\"\"\n",
    "            current_time = time.time()\n",
    "            vertical_speed = 0\n",
    "            current_min_y = min(ls[1], rs[1])\n",
    "            \n",
    "            if self.prev_shoulder_y is not None and self.prev_frame_time is not None:\n",
    "                time_elapsed = current_time - self.prev_frame_time\n",
    "                if time_elapsed > 0:\n",
    "                    vertical_speed = (current_min_y - self.prev_shoulder_y) / time_elapsed\n",
    "                    self.velocity_buffer.append(abs(vertical_speed))\n",
    "            \n",
    "            avg_speed = sum(self.velocity_buffer)/len(self.velocity_buffer) if self.velocity_buffer else 0\n",
    "            speed_cond = avg_speed >= self.VELOCITY_THRESHOLD\n",
    "            \n",
    "            \"\"\" 3. ANGLE CONDITIONS (Paper Section 3.2) \"\"\"\n",
    "            left_leg_angle = self.calculate_angle(lh, lk, la)\n",
    "            right_leg_angle = self.calculate_angle(rh, rk, ra)\n",
    "            leg_angle_cond = min(left_leg_angle, right_leg_angle) < self.LEG_ANGLE_THRESHOLD\n",
    "            \n",
    "            # Torso orientation (not explicitly in paper but mentioned in text)\n",
    "            torso_angle = self.calculate_torso_angle([ls, rs], [lh, rh])\n",
    "            torso_cond = torso_angle > self.TORSO_ANGLE_THRESHOLD\n",
    "            \n",
    "            \"\"\" 4. ASPECT RATIO CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Body orientation ratio: width/height\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            head_to_feet = abs(kp['nose'][1] - max_feet_y)\n",
    "            orientation_ratio = body_width / (head_to_feet + 1e-6)\n",
    "            aspect_cond = orientation_ratio > self.ASPECT_RATIO_THRESHOLD\n",
    "            \n",
    "            \"\"\" FALL DECISION LOGIC (Paper Section 3) \"\"\"\n",
    "            # Combined conditions - at least 2 must be true\n",
    "            conditions_met = sum([height_cond, speed_cond, leg_angle_cond, torso_cond, aspect_cond])\n",
    "            \n",
    "            # State determination\n",
    "            current_state = \"normal\"\n",
    "            conditions_info = []\n",
    "            \n",
    "            if height_cond:\n",
    "                if speed_cond:  # Rapid descent\n",
    "                    current_state = \"falling\"\n",
    "                    self.fall_start_time = current_time\n",
    "                    conditions_info.append(f\"speed:{avg_speed:.1f}px/s\")\n",
    "                elif torso_cond and self.fall_start_time and (current_time - self.fall_start_time < 1.0):\n",
    "                    current_state = \"fallen\"\n",
    "                    conditions_info.append(\"horizontal\")\n",
    "            \n",
    "            if leg_angle_cond:\n",
    "                conditions_info.append(f\"leg_angle:{min(left_leg_angle, right_leg_angle):.0f}°\")\n",
    "            \n",
    "            if aspect_cond:\n",
    "                conditions_info.append(f\"aspect:{orientation_ratio:.2f}\")\n",
    "            \n",
    "            # Final decision with confirmation buffer\n",
    "            is_fall = conditions_met >= 2\n",
    "            self.fall_buffer.append(is_fall)\n",
    "            final_detection = sum(self.fall_buffer) >= 2 if len(self.fall_buffer) >= 1 else is_fall\n",
    "            \n",
    "            if final_detection:\n",
    "                current_state = \"fallen\"\n",
    "                self.fall_detected = True\n",
    "            else:\n",
    "                self.fall_detected = False\n",
    "            \n",
    "            # Update tracking variables\n",
    "            self.prev_keypoints = kp\n",
    "            self.prev_shoulder_y = current_min_y\n",
    "            self.prev_frame_time = current_time\n",
    "            \n",
    "            # Diagnostic information\n",
    "            conditions_info.extend([\n",
    "                f\"height:{'Y' if height_cond else 'N'}\",\n",
    "                f\"speed:{'Y' if speed_cond else 'N'}\",\n",
    "                f\"leg_angle:{'Y' if leg_angle_cond else 'N'}\",\n",
    "                f\"torso:{'Y' if torso_cond else 'N'}\",\n",
    "                f\"aspect:{'Y' if aspect_cond else 'N'}\",\n",
    "                f\"conf:{min(p[2] for p in kp.values()):.2f}\"\n",
    "            ])\n",
    "            \n",
    "            return final_detection, current_state, conditions_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Detection error: {str(e)}\")\n",
    "            return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for fall detection\n",
    "        \n",
    "        Args:\n",
    "            frame: Video frame to process\n",
    "            \n",
    "        Returns:\n",
    "            frame: Processed frame with detections\n",
    "            is_fall: Boolean indicating whether a fall was detected\n",
    "            state: Current state (normal, falling, fallen)\n",
    "            condition_info: List of conditions that triggered the detection\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        orig_image = frame.copy()\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image while maintaining aspect ratio\n",
    "        frame_height, frame_width = orig_image.shape[:2]\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        image = image.float()\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(image)\n",
    "            \n",
    "        # Post-process\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=self.model.yaml['nc'], nkpt=self.model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        \n",
    "        # Convert back to BGR for display\n",
    "        img = image[0].permute(1, 2, 0) * 255\n",
    "        img = img.cpu().numpy().astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize fall status and state for this frame\n",
    "        is_fall = False\n",
    "        current_state = \"normal\"\n",
    "        condition_info = []\n",
    "        \n",
    "        # Process each person detected\n",
    "        for idx in range(output.shape[0]):\n",
    "            # Draw skeleton and keypoints\n",
    "            plot_skeleton_kpts(img, output[idx, 7:].T, 3)\n",
    "            \n",
    "            # Calculate improved bounding box based on keypoints (YouTube approach)\n",
    "            # Find the minimum and maximum x,y coordinates from all keypoints\n",
    "            kpts = output[idx, 7:].reshape(-1, 3)\n",
    "            \n",
    "            # Initialize with first keypoint\n",
    "            x_values = [kpt[0] for kpt in kpts if kpt[2] > 0.5]  # Only use keypoints with confidence > 0.5\n",
    "            y_values = [kpt[1] for kpt in kpts if kpt[2] > 0.5]\n",
    "            \n",
    "            if x_values and y_values:  # Check if we have valid keypoints\n",
    "                xmin, ymin = min(x_values), min(y_values)\n",
    "                xmax, ymax = max(x_values), max(y_values)\n",
    "                \n",
    "                # Add padding to make bounding box a bit larger\n",
    "                padding = 10\n",
    "                xmin = max(0, xmin - padding)\n",
    "                ymin = max(0, ymin - padding)\n",
    "                xmax = xmax + padding\n",
    "                ymax = ymax + padding\n",
    "                \n",
    "                # Calculate aspect ratio for reference (not used in detection)\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                bbox_aspect_ratio = width / height if height > 0 else 0\n",
    "                \n",
    "                # Calculate center\n",
    "                cx = int((xmin + xmax) // 2)\n",
    "                cy = int((ymin + ymax) // 2)\n",
    "                \n",
    "                # For debugging: show aspect ratio on frame\n",
    "                cv2.putText(img, f\"Ratio: {bbox_aspect_ratio:.2f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                # Fallback to original bounding box if no valid keypoints\n",
    "                x1, y1, x2, y2 = output[idx, 0], output[idx, 1], output[idx, 2], output[idx, 3]\n",
    "                xmin, ymin = x1, y1\n",
    "                xmax, ymax = x2, y2\n",
    "                cx, cy = int((x1 + x2) // 2), int((y1 + y2) // 2)\n",
    "            \n",
    "            # Get key points for this person\n",
    "            key_points = output[idx, 7:]\n",
    "            \n",
    "            # Detect fall for this person using enhanced algorithm\n",
    "            person_fall, person_state, person_conditions = self.detect_fall(key_points)\n",
    "            \n",
    "            # If any person is falling, set global fall status\n",
    "            if person_fall:\n",
    "                is_fall = True\n",
    "                current_state = person_state\n",
    "                condition_info = person_conditions\n",
    "                \n",
    "                # Add visual indication of fall\n",
    "                status_text = f\"FALL DETECTED: {person_state.upper()}\"\n",
    "                cv2.putText(img, status_text, (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Draw the bounding box in red for a fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255), 2)\n",
    "                \n",
    "                # For YouTube-style visual, add a colored rectangle at the center\n",
    "                cv2.rectangle(img, (cx-10, cy-10), (cx+10, cy+10), (84, 61, 247), -1)\n",
    "                \n",
    "                # Add condition info to the frame\n",
    "                for i, cond in enumerate(person_conditions[:3]):  # Show first 3 conditions only\n",
    "                    cv2.putText(img, cond, (10, 60 + i*25), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)\n",
    "            else:\n",
    "                # Draw normal bounding box in green for no fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 1)\n",
    "                \n",
    "                # Show normal state\n",
    "                cv2.putText(img, f\"State: {person_state}\", (10, 60), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return img, is_fall, current_state, condition_info\n",
    "\n",
    "def run_fall_detection(poseweights='yolov7-w6-pose.pt', source='pose.mp4', device='cpu', display=True, save_output=True, \n",
    "                     save_false_detections=True, detector=None, batch_mode=False):\n",
    "    \"\"\"\n",
    "    Run fall detection on a video or webcam feed\n",
    "    \n",
    "    Args:\n",
    "        poseweights: Path to the YOLOv7 pose weights\n",
    "        source: Path to video file or webcam ID (0, 1, etc.)\n",
    "        device: Device to run inference on ('cpu' or '0', '1', etc. for GPU)\n",
    "        display: Whether to show video with detections in real-time\n",
    "        save_output: Whether to save the output video\n",
    "        save_false_detections: Whether to save false detections to a file\n",
    "        detector: Optional pre-initialized FallDetector instance (for batch processing)\n",
    "        batch_mode: Whether this is being run as part of a batch process (suppresses some output)\n",
    "    \"\"\"\n",
    "    # Initialize the fall detector or use the provided one\n",
    "    if detector is None:\n",
    "        detector = FallDetector(poseweights=poseweights, device=device)\n",
    "    \n",
    "    # Parse the input source\n",
    "    input_path = source\n",
    "    if source.isnumeric():\n",
    "        input_path = int(source)\n",
    "    \n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return {\n",
    "            'frames_processed': 0,\n",
    "            'average_fps': 0,\n",
    "            'false_detections': 0\n",
    "        }\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Check if we're processing a Le2i dataset video\n",
    "    is_le2i = False\n",
    "    annotation_file = None\n",
    "    ground_truth_fall_frames = []\n",
    "    expected_fall_type = None\n",
    "    \n",
    "    # Check if it's a Le2i video by looking at the path\n",
    "    if isinstance(input_path, str) and \"Le2i_Sorted\" in input_path and \"Videos\" in input_path:\n",
    "        is_le2i = True\n",
    "        \n",
    "        # Create a dedicated output folder for Le2i dataset results\n",
    "        le2i_output_dir = os.path.join('output', 'le2i_results')\n",
    "        os.makedirs(le2i_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract environment and Fall/Non Fall type from path for folder organization\n",
    "        path_parts = input_path.split(os.sep)\n",
    "        try:\n",
    "            # Find Fall or Non Fall in the path\n",
    "            fall_idx = -1\n",
    "            for i, part in enumerate(path_parts):\n",
    "                if part in [\"Fall\", \"Non Fall\"]:\n",
    "                    fall_idx = i\n",
    "                    expected_fall_type = part  # Save whether this is a Fall or Non Fall video\n",
    "                    break\n",
    "            \n",
    "            if fall_idx >= 0 and fall_idx + 1 < len(path_parts):\n",
    "                fall_type = path_parts[fall_idx]\n",
    "                env_type = path_parts[fall_idx + 1]\n",
    "                \n",
    "                # Create subfolder for this environment and fall type\n",
    "                env_fall_dir = os.path.join(le2i_output_dir, f\"{env_type}_{fall_type}\")\n",
    "                os.makedirs(env_fall_dir, exist_ok=True)\n",
    "            else:\n",
    "                env_fall_dir = le2i_output_dir\n",
    "        except:\n",
    "            env_fall_dir = le2i_output_dir\n",
    "            \n",
    "        video_filename = os.path.basename(input_path)\n",
    "        \n",
    "        # Construct path to annotation file by replacing Videos with Annotation_files and changing extension\n",
    "        annotation_path = input_path.replace(\"Videos\", \"Annotation_files\").rsplit(\".\", 1)[0] + \".txt\"\n",
    "        \n",
    "        # Check if annotation file exists\n",
    "        if os.path.exists(annotation_path):\n",
    "            try:\n",
    "                with open(annotation_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    \n",
    "                    # The first 2 lines might be metadata (number of frames, etc.)\n",
    "                    # Skip them if they don't contain fall annotations\n",
    "                    data_lines = []\n",
    "                    for line in lines:\n",
    "                        # Try to parse as comma-separated values\n",
    "                        if ',' in line:\n",
    "                            data_lines.append(line)\n",
    "                        # Also try space-separated values\n",
    "                        elif len(line.strip().split()) >= 2:\n",
    "                            # Convert space-separated to comma-separated\n",
    "                            values = line.strip().split()\n",
    "                            data_lines.append(','.join(values))\n",
    "                    \n",
    "                    for line in data_lines:\n",
    "                        parts = line.strip().split(',')\n",
    "                        if len(parts) >= 2:\n",
    "                            try:\n",
    "                                # Format can be \"frame_number,label,x,y,width,height\" or similar\n",
    "                                frame_num = int(parts[0])\n",
    "                                label = int(parts[1])  # 1 for Fall, other values for no fall\n",
    "                                \n",
    "                                # If the label indicates a fall (usually 1, 7, or 8 in Le2i), \n",
    "                                # add this frame to ground truth fall frames\n",
    "                                if label in [1, 7, 8]:  # Common fall labels in Le2i\n",
    "                                    ground_truth_fall_frames.append(frame_num)\n",
    "                            except (ValueError, IndexError):\n",
    "                                # Skip lines that can't be parsed correctly\n",
    "                                continue\n",
    "                \n",
    "                if not batch_mode:\n",
    "                    print(f\"Loaded {len(ground_truth_fall_frames)} annotated fall frames from {annotation_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotation file: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Annotation file not found at {annotation_path}\")\n",
    "            is_le2i = False\n",
    "    \n",
    "    # Setup output video writer if requested\n",
    "    out = None\n",
    "    if save_output:\n",
    "        if isinstance(input_path, int):\n",
    "            # For webcam\n",
    "            output_path = os.path.join('output', f\"webcam_fall_detection.mp4\")\n",
    "        elif is_le2i:\n",
    "            # For Le2i dataset videos, save to the dedicated folder\n",
    "            video_name = os.path.basename(input_path).split('.')[0]\n",
    "            output_path = os.path.join(env_fall_dir, f\"{video_name}_fall_detection.mp4\")\n",
    "        else:\n",
    "            # For regular video files\n",
    "            filename = os.path.basename(input_path).split('.')[0]\n",
    "            output_path = os.path.join('output', f\"{filename}_fall_detection.mp4\")\n",
    "        \n",
    "        # Create VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "        if not batch_mode:\n",
    "            print(f\"Output will be saved to: {output_path}\")\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    total_fps = 0\n",
    "    \n",
    "    # For performance tracking\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    \n",
    "    # For false detection tracking\n",
    "    false_detections = []\n",
    "    \n",
    "    if not batch_mode:\n",
    "        print(f\"Starting fall detection on {os.path.basename(input_path) if isinstance(input_path, str) else 'webcam'}...\")\n",
    "        print(f\"Total frames: {total_frames}\")\n",
    "    \n",
    "    # Process the video frames with a timeout mechanism to prevent hanging\n",
    "    start_time = time.time()\n",
    "    max_process_time = 300  # 5 minutes max per video\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Print progress periodically if not in batch mode\n",
    "        if not batch_mode and frame_count % 10 == 0:\n",
    "            print(f\"Processing frame {frame_count}/{total_frames if total_frames > 0 else 'unknown'}\")\n",
    "        \n",
    "        # Check if we've been processing too long\n",
    "        current_time = time.time()\n",
    "        if current_time - start_time > max_process_time:\n",
    "            print(f\"Warning: Processing time limit reached ({max_process_time}s). Stopping early.\")\n",
    "            break\n",
    "        \n",
    "        # Process frame for fall detection\n",
    "        try:\n",
    "            frame_start_time = time.time()\n",
    "            processed_frame, is_fall, current_state, condition_info = detector.process_frame(frame)\n",
    "            frame_end_time = time.time()\n",
    "            \n",
    "            # Calculate FPS\n",
    "            processing_fps = 1 / (frame_end_time - frame_start_time)\n",
    "            total_fps += processing_fps\n",
    "            \n",
    "            # Resize processed frame to match original dimensions for display and saving\n",
    "            processed_frame_resized = cv2.resize(processed_frame, (frame_width, frame_height))\n",
    "            \n",
    "            # Add FPS info and frame count\n",
    "            cv2.putText(processed_frame_resized, f\"FPS: {processing_fps:.2f}\", (frame_width - 150, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(processed_frame_resized, f\"Frame: {frame_count}\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Check for false detections if we have annotation data\n",
    "            if is_le2i and ground_truth_fall_frames:\n",
    "                is_ground_truth_fall = frame_count in ground_truth_fall_frames\n",
    "                \n",
    "                # Update performance metrics\n",
    "                if is_fall and is_ground_truth_fall:\n",
    "                    true_positives += 1\n",
    "                    outcome_text = \"TRUE POSITIVE\"\n",
    "                    color = (0, 255, 0)  # Green for TP\n",
    "                elif is_fall and not is_ground_truth_fall:\n",
    "                    false_positives += 1\n",
    "                    outcome_text = \"FALSE POSITIVE\"\n",
    "                    color = (0, 0, 255)  # Red for FP\n",
    "                elif not is_fall and is_ground_truth_fall:\n",
    "                    false_negatives += 1\n",
    "                    outcome_text = \"FALSE NEGATIVE\"\n",
    "                    color = (255, 0, 0)  # Blue for FN\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "                    outcome_text = \"TRUE NEGATIVE\"\n",
    "                    color = (255, 255, 0)  # Cyan for TN\n",
    "                \n",
    "                # Add outcome label to frame\n",
    "                cv2.putText(processed_frame_resized, outcome_text, (frame_width - 250, 60), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                \n",
    "                # If there's a mismatch between detection and ground truth, record it\n",
    "                if is_fall != is_ground_truth_fall:\n",
    "                    false_detections.append({\n",
    "                        'frame': frame_count,\n",
    "                        'predicted': 'Fall' if is_fall else 'No Fall',\n",
    "                        'actual': 'Fall' if is_ground_truth_fall else 'No Fall',\n",
    "                        'type': 'False Positive' if is_fall and not is_ground_truth_fall else 'False Negative'\n",
    "                    })\n",
    "            \n",
    "            # Display the frame if requested\n",
    "            if display:\n",
    "                cv2.imshow('Fall Detection', processed_frame_resized)\n",
    "                \n",
    "                # Exit on 'q' press\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('n') and batch_mode:\n",
    "                    # In batch mode, allow 'n' to skip to the next video\n",
    "                    print(\"Skipping to next video...\")\n",
    "                    break\n",
    "            \n",
    "            # Save frame to output video if requested\n",
    "            if save_output and out is not None:\n",
    "                out.write(processed_frame_resized)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "            # Continue to next frame\n",
    "            continue\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if save_output and out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print statistics if not in batch mode\n",
    "    if frame_count > 0 and not batch_mode:\n",
    "        avg_fps = total_fps / frame_count\n",
    "        print(f\"Processed {frame_count} frames\")\n",
    "        print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "        if save_output:\n",
    "            print(f\"Output saved to: {output_path}\")\n",
    "        \n",
    "        # Print detection metrics\n",
    "        if is_le2i:\n",
    "            print(\"\\nDetection Metrics:\")\n",
    "            print(f\"True Positives: {true_positives}\")\n",
    "            print(f\"True Negatives: {true_negatives}\")\n",
    "            print(f\"False Positives: {false_positives}\")\n",
    "            print(f\"False Negatives: {false_negatives}\")\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if (true_positives + true_negatives + false_positives + false_negatives) > 0 else 0\n",
    "            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "            print(f\"Precision: {precision*100:.2f}%\")\n",
    "            print(f\"Recall: {recall*100:.2f}%\")\n",
    "            print(f\"F1 Score: {f1_score*100:.2f}%\")\n",
    "    \n",
    "    # Create false detections file path\n",
    "    false_detections_path = None\n",
    "    \n",
    "    # Save false detections to file if we processed Le2i data\n",
    "    if is_le2i and save_false_detections and (false_positives > 0 or false_negatives > 0):\n",
    "        # Create the false_detections.txt in the appropriate folder\n",
    "        if 'env_fall_dir' in locals():\n",
    "            false_detections_path = os.path.join(env_fall_dir, 'false_detections.txt')\n",
    "        else:\n",
    "            false_detections_path = os.path.join(le2i_output_dir, 'false_detections.txt')\n",
    "        \n",
    "        try:\n",
    "            with open(false_detections_path, 'a') as f:\n",
    "                f.write(f\"\\n--- False Detections for {os.path.basename(input_path)} ---\\n\")\n",
    "                for detection in false_detections:\n",
    "                    f.write(f\"Frame {detection['frame']}: {detection['type']} - Predicted: {detection['predicted']}, Actual: {detection['actual']}\\n\")\n",
    "            if not batch_mode:\n",
    "                print(f\"False detections saved to: {false_detections_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving false detections: {str(e)}\")\n",
    "    \n",
    "    # Calculate average FPS\n",
    "    avg_fps = total_fps / frame_count if frame_count > 0 else 0\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if (true_positives + true_negatives + false_positives + false_negatives) > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Return statistics\n",
    "    return {\n",
    "        'frames_processed': frame_count,\n",
    "        'average_fps': avg_fps,\n",
    "        'false_detections': len(false_detections) if is_le2i else 0,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives, \n",
    "        'true_positives': true_positives,\n",
    "        'true_negatives': true_negatives,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'output_path': output_path if save_output else None,\n",
    "        'false_detections_path': false_detections_path\n",
    "    }\n",
    "\n",
    "def run_interactive():\n",
    "    \"\"\"\n",
    "    Interactive function to run fall detection with user input\n",
    "    \"\"\"\n",
    "    # Get the weights file\n",
    "    poseweights = input(\"Enter path to weights file [default: yolov7-w6-pose.pt]: \") or \"yolov7-w6-pose.pt\"\n",
    "    \n",
    "    # Get device type\n",
    "    use_gpu = input(\"Use GPU? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    if use_gpu == \"y\":\n",
    "        device = input(\"Enter GPU device ID [default: 0]: \") or \"0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Get source type\n",
    "    print(\"\\nSelect input source:\")\n",
    "    print(\"1: Video file\")\n",
    "    print(\"2: Webcam\")\n",
    "    print(\"3: Single video from Le2i dataset\")\n",
    "    print(\"4: Process all Le2i dataset videos\")\n",
    "    source_choice = input(\"Enter choice [1/2/3/4]: \")\n",
    "    \n",
    "    if source_choice == \"1\":\n",
    "        # Video file\n",
    "        default_video = \"sample_video.mp4\"\n",
    "        source = input(f\"Enter video file path [default: {default_video}]: \") or default_video\n",
    "        # Ask if user wants to display the processed video in real-time\n",
    "        display_video = input(\"Display video with pose estimation in real-time? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        # Ask if user wants to save the output video\n",
    "        save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        print(f\"\\nRunning fall detection with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Source: {source}\")\n",
    "        print(f\"- Display: {'Yes' if display_video == 'y' else 'No'}\")\n",
    "        print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # Run the model\n",
    "            run_with_display = (display_video == \"y\")\n",
    "            save_output = (save_video == \"y\")\n",
    "            \n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Run fall detection\n",
    "            run_fall_detection(\n",
    "                poseweights=poseweights,\n",
    "                source=source,\n",
    "                device=device,\n",
    "                display=run_with_display,\n",
    "                save_output=save_output\n",
    "            )\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    elif source_choice == \"2\":\n",
    "        # Webcam\n",
    "        cam_id = input(\"Enter webcam ID [default: 0]: \") or \"0\"\n",
    "        source = cam_id\n",
    "        \n",
    "        # Ask if user wants to save the output video\n",
    "        save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        print(f\"\\nRunning fall detection with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Source: Webcam {source}\")\n",
    "        print(f\"- Display: Yes\")  # Always display for webcam\n",
    "        print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Run fall detection\n",
    "            run_fall_detection(\n",
    "                poseweights=poseweights,\n",
    "                source=source,\n",
    "                device=device,\n",
    "                display=True,  # Always display for webcam\n",
    "                save_output=(save_video == \"y\")\n",
    "            )\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    elif source_choice == \"3\":\n",
    "        # Single video from Le2i dataset\n",
    "        default_dataset_path = \"datasets\"\n",
    "        dataset_path = input(f\"Enter dataset root path [default: {default_dataset_path}]: \") or default_dataset_path\n",
    "        \n",
    "        # Check if the dataset path exists\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: Dataset path '{dataset_path}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i dataset structure\n",
    "        le2i_path = os.path.join(dataset_path, \"le2i\")\n",
    "        if not os.path.exists(le2i_path):\n",
    "            print(f\"Error: Le2i dataset not found at {le2i_path}\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i_Sorted structure\n",
    "        le2i_sorted_path = os.path.join(le2i_path, \"Le2i_Sorted\")\n",
    "        is_sorted = os.path.exists(le2i_sorted_path)\n",
    "        \n",
    "        if is_sorted:\n",
    "            print(f\"Found Le2i dataset with sorted structure (Fall/Non Fall folders)\")\n",
    "            # Get environment folder\n",
    "            env_folders = [\"Coffee_room_01\", \"Coffee_room_02\", \"Home_01\", \"Home_02\", \"Lecture_room\", \"Office\"]\n",
    "            print(\"Choose environment folder:\")\n",
    "            for i, env in enumerate(env_folders, 1):\n",
    "                print(f\"{i}: {env}\")\n",
    "            env_choice = input(\"Enter choice [1-6, default: 1]: \") or \"1\"\n",
    "            try:\n",
    "                env_index = int(env_choice) - 1\n",
    "                if 0 <= env_index < len(env_folders):\n",
    "                    env_folder = env_folders[env_index]\n",
    "                else:\n",
    "                    print(\"Invalid choice. Using Coffee_room_01.\")\n",
    "                    env_folder = env_folders[0]\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Using Coffee_room_01.\")\n",
    "                env_folder = env_folders[0]\n",
    "                \n",
    "            # Choose Fall or Non Fall\n",
    "            fall_type = input(\"Choose 'Fall' or 'Non Fall' [default: Fall]: \").strip() or \"Fall\"\n",
    "            if fall_type.lower() not in [\"fall\", \"non fall\"]:\n",
    "                print(\"Invalid choice. Using 'Fall'.\")\n",
    "                fall_type = \"Fall\"\n",
    "                \n",
    "            # Construct path to Videos folder\n",
    "            videos_folder = os.path.join(le2i_path, \"Le2i_Sorted\", fall_type, env_folder, \"Videos\")\n",
    "            if not os.path.exists(videos_folder):\n",
    "                print(f\"Error: Videos folder not found at {videos_folder}\")\n",
    "                return\n",
    "                \n",
    "            # List available videos\n",
    "            video_files = [f for f in os.listdir(videos_folder) \n",
    "                         if f.endswith(('.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV'))]\n",
    "            \n",
    "            if not video_files:\n",
    "                print(f\"Error: No video files found in {videos_folder}\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Found {len(video_files)} video files.\")\n",
    "            print(\"Choose video file:\")\n",
    "            for i, video in enumerate(video_files, 1):\n",
    "                print(f\"{i}: {video}\")\n",
    "                \n",
    "            video_choice = input(f\"Enter choice [1-{len(video_files)}, default: 1]: \") or \"1\"\n",
    "            try:\n",
    "                video_index = int(video_choice) - 1\n",
    "                if 0 <= video_index < len(video_files):\n",
    "                    video_file = video_files[video_index]\n",
    "                else:\n",
    "                    print(f\"Invalid choice. Using {video_files[0]}.\")\n",
    "                    video_file = video_files[0]\n",
    "            except ValueError:\n",
    "                print(f\"Invalid input. Using {video_files[0]}.\")\n",
    "                video_file = video_files[0]\n",
    "                \n",
    "            # Construct full path to video file\n",
    "            source = os.path.join(videos_folder, video_file)\n",
    "        else:\n",
    "            print(\"Le2i dataset with traditional structure not supported for this option.\")\n",
    "            return\n",
    "            \n",
    "        # Ask if user wants to display the processed video in real-time - default to Yes for batch process\n",
    "        display_video = input(\"Display video with pose estimation in real-time? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        # Ask if user wants to save the output video\n",
    "        save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        # Ask if user wants to save false detections to a file\n",
    "        save_false_detections = input(\"Save false detections to a file? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        print(f\"\\nRunning fall detection with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Source: {source}\")\n",
    "        print(f\"- Display: {'Yes' if display_video == 'y' else 'No'}\")\n",
    "        print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "        print(f\"- Save false detections: {'Yes' if save_false_detections == 'y' else 'No'}\")\n",
    "        print(f\"- Fall video comparison: Enabled (will check annotations)\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Run fall detection using the same function as the other options\n",
    "            results = run_fall_detection(\n",
    "                poseweights=poseweights,\n",
    "                source=source,\n",
    "                device=device,\n",
    "                display=(display_video == \"y\"),  # Display parameter based on user input\n",
    "                save_output=(save_video == \"y\"),\n",
    "                save_false_detections=(save_false_detections == \"y\")\n",
    "            )\n",
    "            \n",
    "            # Display summarized results\n",
    "            if results:\n",
    "                print(\"\\nProcessing Summary:\")\n",
    "                print(f\"Processed {results['frames_processed']} frames\")\n",
    "                print(f\"Average FPS: {results['average_fps']:.2f}\")\n",
    "                print(f\"False detections: {results['false_detections']}\")\n",
    "                \n",
    "                if results['false_detections'] > 0 and save_false_detections == 'y':\n",
    "                    print(f\"False detection details saved to: {results['false_detections_path']}\")\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    elif source_choice == \"4\":\n",
    "        # Process all Le2i dataset videos\n",
    "        default_dataset_path = \"datasets\"\n",
    "        dataset_path = input(f\"Enter dataset root path [default: {default_dataset_path}]: \") or default_dataset_path\n",
    "        \n",
    "        # Check if the dataset path exists\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: Dataset path '{dataset_path}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i dataset structure\n",
    "        le2i_path = os.path.join(dataset_path, \"le2i\")\n",
    "        if not os.path.exists(le2i_path):\n",
    "            print(f\"Error: Le2i dataset not found at {le2i_path}\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i_Sorted structure\n",
    "        le2i_sorted_path = os.path.join(le2i_path, \"Le2i_Sorted\")\n",
    "        is_sorted = os.path.exists(le2i_sorted_path)\n",
    "        \n",
    "        if not is_sorted:\n",
    "            print(\"Error: Le2i dataset with sorted structure (Fall/Non Fall folders) not found.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found Le2i dataset with sorted structure at {le2i_sorted_path}\")\n",
    "        \n",
    "        # Ask if user wants to display the processed videos in real-time\n",
    "        display_videos = input(\"Display videos with pose estimation in real-time? (y/n) [default: n]: \").lower() or \"n\"\n",
    "        \n",
    "        # Ask if user wants to save the output videos\n",
    "        save_videos = input(\"Save output videos? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        # Ask if user wants to save false detections to a file\n",
    "        save_false_detections = input(\"Save false detections to a file? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        # Create summary stats file\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        summary_path = os.path.join('output', 'le2i_results', f'batch_summary_{timestamp}.txt')\n",
    "        os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nRunning batch processing of all Le2i dataset videos with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Display: {'Yes' if display_videos == 'y' else 'No'}\")\n",
    "        print(f\"- Save outputs: {'Yes' if save_videos == 'y' else 'No'}\")\n",
    "        print(f\"- Save false detections: {'Yes' if save_false_detections == 'y' else 'No'}\")\n",
    "        print(f\"- Summary will be saved to: {summary_path}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Initialize FallDetector once for all videos\n",
    "            detector = FallDetector(poseweights=poseweights, device=device)\n",
    "            \n",
    "            # Get list of all environment folders\n",
    "            fall_folders = ['Fall', 'Non Fall']\n",
    "            env_folders = [\"Coffee_room_01\", \"Coffee_room_02\", \"Home_01\", \"Home_02\", \"Lecture_room\", \"Office\"]\n",
    "            \n",
    "            # Initialize counters for summary\n",
    "            total_videos = 0\n",
    "            total_videos_processed = 0\n",
    "            total_frames = 0\n",
    "            total_falls_detected = 0\n",
    "            total_false_positives = 0\n",
    "            total_false_negatives = 0\n",
    "            total_true_positives = 0\n",
    "            total_true_negatives = 0\n",
    "            total_processing_time = 0\n",
    "            \n",
    "            # Open summary file\n",
    "            with open(summary_path, 'w') as summary_file:\n",
    "                summary_file.write(f\"Le2i Dataset Batch Processing Summary - {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                summary_file.write(\"=\" * 80 + \"\\n\\n\")\n",
    "                \n",
    "                # Process all fall types\n",
    "                for fall_type in fall_folders:\n",
    "                    summary_file.write(f\"\\n{fall_type} Videos:\\n\")\n",
    "                    summary_file.write(\"-\" * 50 + \"\\n\")\n",
    "                    \n",
    "                    # Process all environment folders\n",
    "                    for env_folder in env_folders:\n",
    "                        videos_folder = os.path.join(le2i_path, \"Le2i_Sorted\", fall_type, env_folder, \"Videos\")\n",
    "                        \n",
    "                        # Skip if folder doesn't exist\n",
    "                        if not os.path.exists(videos_folder):\n",
    "                            continue\n",
    "                        \n",
    "                        # Get video files\n",
    "                        video_files = [f for f in os.listdir(videos_folder) \n",
    "                                     if f.endswith(('.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV'))]\n",
    "                        \n",
    "                        if not video_files:\n",
    "                            continue\n",
    "                        \n",
    "                        total_videos += len(video_files)\n",
    "                        summary_file.write(f\"\\nEnvironment: {env_folder} - {len(video_files)} videos\\n\")\n",
    "                        summary_file.flush()  # Flush to ensure progress is saved\n",
    "                        \n",
    "                        # Process each video\n",
    "                        for video_file in video_files:\n",
    "                            source = os.path.join(videos_folder, video_file)\n",
    "                            print(f\"\\nProcessing: {fall_type}/{env_folder}/{video_file}\")\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            \n",
    "                            # Run fall detection\n",
    "                            try:\n",
    "                                results = run_fall_detection(\n",
    "                                    detector=detector,  # Reuse the same detector\n",
    "                                    source=source,\n",
    "                                    device=device,\n",
    "                                    display=(display_videos == \"y\"),\n",
    "                                    save_output=(save_videos == \"y\"),\n",
    "                                    save_false_detections=(save_false_detections == \"y\"),\n",
    "                                    batch_mode=True  # Enable batch mode to suppress individual metrics\n",
    "                                )\n",
    "                                \n",
    "                                end_time = time.time()\n",
    "                                processing_time = end_time - start_time\n",
    "                                \n",
    "                                # Check if processing was actually completed\n",
    "                                if results['frames_processed'] > 0:\n",
    "                                    total_videos_processed += 1\n",
    "                                    total_frames += results['frames_processed']\n",
    "                                    total_processing_time += processing_time\n",
    "                                    \n",
    "                                    # Update metrics counters\n",
    "                                    total_false_positives += results['false_positives']\n",
    "                                    total_false_negatives += results['false_negatives']\n",
    "                                    total_true_positives += results['true_positives']\n",
    "                                    total_true_negatives += results['true_negatives']\n",
    "                                    \n",
    "                                    if fall_type == 'Fall':\n",
    "                                        total_falls_detected += results['true_positives']\n",
    "                                    \n",
    "                                    # Write results to summary\n",
    "                                    summary_file.write(f\"  - {video_file}: {results['frames_processed']} frames, \")\n",
    "                                    summary_file.write(f\"FPS: {results['average_fps']:.2f}\")\n",
    "                                    \n",
    "                                    if 'accuracy' in results:\n",
    "                                        summary_file.write(f\", Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "                                    \n",
    "                                    if fall_type == 'Fall':\n",
    "                                        summary_file.write(f\", False Negatives: {results['false_negatives']}, \")\n",
    "                                        summary_file.write(f\"True Positives: {results['true_positives']}\")\n",
    "                                    else:\n",
    "                                        summary_file.write(f\", False Positives: {results['false_positives']}, \")\n",
    "                                        summary_file.write(f\"True Negatives: {results['true_negatives']}\")\n",
    "                                    \n",
    "                                    summary_file.write(\"\\n\")\n",
    "                                    summary_file.flush()  # Flush to ensure progress is saved\n",
    "                                else:\n",
    "                                    print(f\"Warning: No frames were processed for {video_file}. Skipping.\")\n",
    "                                    summary_file.write(f\"  - {video_file}: SKIPPED - No frames processed\\n\")\n",
    "                                    summary_file.flush()\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {video_file}: {str(e)}\")\n",
    "                                summary_file.write(f\"  - {video_file}: ERROR - {str(e)}\\n\")\n",
    "                                summary_file.flush()\n",
    "                \n",
    "                # Calculate overall metrics\n",
    "                if total_frames > 0:\n",
    "                    overall_accuracy = (total_true_positives + total_true_negatives) / (total_true_positives + total_true_negatives + total_false_positives + total_false_negatives) if (total_true_positives + total_true_negatives + total_false_positives + total_false_negatives) > 0 else 0\n",
    "                    overall_precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "                    overall_recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "                    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "                    \n",
    "                    # Write overall summary\n",
    "                    summary_file.write(\"\\n\\nOverall Summary:\\n\")\n",
    "                    summary_file.write(\"=\" * 50 + \"\\n\")\n",
    "                    summary_file.write(f\"Total videos: {total_videos}\\n\")\n",
    "                    summary_file.write(f\"Videos processed: {total_videos_processed}\\n\")\n",
    "                    summary_file.write(f\"Total frames processed: {total_frames}\\n\")\n",
    "                    summary_file.write(f\"Total processing time: {total_processing_time:.2f} seconds\\n\")\n",
    "                    summary_file.write(f\"Average FPS: {total_frames/total_processing_time if total_processing_time > 0 else 0:.2f}\\n\")\n",
    "                    summary_file.write(f\"Total true positives: {total_true_positives}\\n\")\n",
    "                    summary_file.write(f\"Total true negatives: {total_true_negatives}\\n\")\n",
    "                    summary_file.write(f\"Total false positives: {total_false_positives}\\n\")\n",
    "                    summary_file.write(f\"Total false negatives: {total_false_negatives}\\n\")\n",
    "                    summary_file.write(f\"Overall accuracy: {overall_accuracy*100:.2f}%\\n\")\n",
    "                    summary_file.write(f\"Overall precision: {overall_precision*100:.2f}%\\n\")\n",
    "                    summary_file.write(f\"Overall recall: {overall_recall*100:.2f}%\\n\")\n",
    "                    summary_file.write(f\"Overall F1 score: {overall_f1*100:.2f}%\\n\")\n",
    "            \n",
    "            # Print overall summary to console\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"BATCH PROCESSING COMPLETE\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Videos processed: {total_videos_processed}/{total_videos}\")\n",
    "            print(f\"Total frames processed: {total_frames}\")\n",
    "            print(f\"Total processing time: {total_processing_time:.2f} seconds\")\n",
    "            print(f\"Average FPS: {total_frames/total_processing_time if total_processing_time > 0 else 0:.2f}\")\n",
    "            print(f\"Overall accuracy: {overall_accuracy*100:.2f}%\")\n",
    "            print(f\"Overall precision: {overall_precision*100:.2f}%\")\n",
    "            print(f\"Overall recall: {overall_recall*100:.2f}%\")\n",
    "            print(f\"Overall F1 score: {overall_f1*100:.2f}%\")\n",
    "            print(f\"\\nDetailed summary saved to: {summary_path}\")\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Please run again and select a valid option.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Run interactively\n",
    "    run_interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
