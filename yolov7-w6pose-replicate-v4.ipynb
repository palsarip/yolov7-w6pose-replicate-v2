{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082252c-c3fb-4b14-929c-462607669be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c95daf-7329-431d-823c-c75369547fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to weights file [default: yolov7-w6-pose.pt]:  \n",
      "Use GPU? (y/n) [default: y]:  \n",
      "Enter GPU device ID [default: 0]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select input source:\n",
      "1: Video file\n",
      "2: Webcam\n",
      "3: Batch process (Le2i dataset)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice [1/2/3]:  1\n",
      "Enter video file path [default: sample_video.mp4]:  datasets/le2i/Le2i_Sorted/Fall/Home_01/Videos/video (12).avi\n",
      "Display video with pose estimation in real-time? (y/n) [default: y]:  y\n",
      "Save output video? (y/n) [default: y]:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running fall detection with:\n",
      "- Weights: yolov7-w6-pose.pt\n",
      "- Device: 0\n",
      "- Source: datasets/le2i/Le2i_Sorted/Fall/Home_01/Videos/video (12).avi\n",
      "- Display: Yes\n",
      "- Save output: Yes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Confirm? (y/n) [default: y]:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from yolov7-w6-pose.pt, 161.1MB\n",
      "Initializing Fall Detector with weights: yolov7-w6-pose.pt on device: 0\n",
      "Fusing layers... \n",
      "Output will be saved to: output\\video (12)_fall_detection.mp4\n",
      "Starting fall detection...\n",
      "Processing frame 1\n",
      "Processing frame 2\n",
      "Processing frame 3\n",
      "Processing frame 4\n",
      "Processing frame 5\n",
      "Processing frame 6\n",
      "Processing frame 7\n",
      "Processing frame 8\n",
      "Processing frame 9\n",
      "Processing frame 10\n",
      "Processing frame 11\n",
      "Processing frame 12\n",
      "Processing frame 13\n",
      "Processing frame 14\n",
      "Processing frame 15\n",
      "Processing frame 16\n",
      "Processing frame 17\n",
      "Processing frame 18\n",
      "Processing frame 19\n",
      "Processing frame 20\n",
      "Processing frame 21\n",
      "Processing frame 22\n",
      "Processing frame 23\n",
      "Processing frame 24\n",
      "Processing frame 25\n",
      "Processing frame 26\n",
      "Processing frame 27\n",
      "Processing frame 28\n",
      "Processing frame 29\n",
      "Processing frame 30\n",
      "Processing frame 31\n",
      "Processing frame 32\n",
      "Processing frame 33\n",
      "Processing frame 34\n",
      "Processing frame 35\n",
      "Processing frame 36\n",
      "Processing frame 37\n",
      "Processing frame 38\n",
      "Processing frame 39\n",
      "Processing frame 40\n",
      "Processing frame 41\n",
      "Processing frame 42\n",
      "Processing frame 43\n",
      "Processing frame 44\n",
      "Processing frame 45\n",
      "Processing frame 46\n",
      "Processing frame 47\n",
      "Processing frame 48\n",
      "Processing frame 49\n",
      "Processing frame 50\n",
      "Processing frame 51\n",
      "Processing frame 52\n",
      "Processing frame 53\n",
      "Processing frame 54\n",
      "Processing frame 55\n",
      "Processing frame 56\n",
      "Processing frame 57\n",
      "Processing frame 58\n",
      "Processing frame 59\n",
      "Processing frame 60\n",
      "Processing frame 61\n",
      "Processing frame 62\n",
      "Processing frame 63\n",
      "Processing frame 64\n",
      "Processing frame 65\n",
      "Processing frame 66\n",
      "Processing frame 67\n",
      "Processing frame 68\n",
      "Processing frame 69\n",
      "Processing frame 70\n",
      "Processing frame 71\n",
      "Processing frame 72\n",
      "Processing frame 73\n",
      "Processing frame 74\n",
      "Processing frame 75\n",
      "Processing frame 76\n",
      "Processing frame 77\n",
      "Processing frame 78\n",
      "Processing frame 79\n",
      "Processing frame 80\n",
      "Processing frame 81\n",
      "Processing frame 82\n",
      "Processing frame 83\n",
      "Processing frame 84\n",
      "Processing frame 85\n",
      "Processing frame 86\n",
      "Processing frame 87\n",
      "Processing frame 88\n",
      "Processing frame 89\n",
      "Processing frame 90\n",
      "Processing frame 91\n",
      "Processing frame 92\n",
      "Processing frame 93\n",
      "Processing frame 94\n",
      "Processing frame 95\n",
      "Processing frame 96\n",
      "Processing frame 97\n",
      "Processing frame 98\n",
      "Processing frame 99\n",
      "Processing frame 100\n",
      "Processing frame 101\n",
      "Processing frame 102\n",
      "Processing frame 103\n",
      "Processing frame 104\n",
      "Processing frame 105\n",
      "Processing frame 106\n",
      "Processing frame 107\n",
      "Processing frame 108\n",
      "Processing frame 109\n",
      "Processing frame 110\n",
      "Processing frame 111\n",
      "Processing frame 112\n",
      "Processing frame 113\n",
      "Processing frame 114\n",
      "Processing frame 115\n",
      "Processing frame 116\n",
      "Processing frame 117\n",
      "Processing frame 118\n",
      "Processing frame 119\n",
      "Processing frame 120\n",
      "Processing frame 121\n",
      "Processing frame 122\n",
      "Processing frame 123\n",
      "Processing frame 124\n",
      "Processing frame 125\n",
      "Processing frame 126\n",
      "Processing frame 127\n",
      "Processing frame 128\n",
      "Processing frame 129\n",
      "Processing frame 130\n",
      "Processing frame 131\n",
      "Processing frame 132\n",
      "Processing frame 133\n",
      "Processing frame 134\n",
      "Processing frame 135\n",
      "Processing frame 136\n",
      "Processing frame 137\n",
      "Processing frame 138\n",
      "Processing frame 139\n",
      "Processing frame 140\n",
      "Processing frame 141\n",
      "Processing frame 142\n",
      "Processing frame 143\n",
      "Processing frame 144\n",
      "Processing frame 145\n",
      "Processing frame 146\n",
      "Processing frame 147\n",
      "Processing frame 148\n",
      "Processing frame 149\n",
      "Processing frame 150\n",
      "Processing frame 151\n",
      "Processing frame 152\n",
      "Processing frame 153\n",
      "Processing frame 154\n",
      "Processing frame 155\n",
      "Processing frame 156\n",
      "Processing frame 157\n",
      "Processing frame 158\n",
      "Processing frame 159\n",
      "Processing frame 160\n",
      "Processing frame 161\n",
      "Processing frame 162\n",
      "Processing frame 163\n",
      "Processing frame 164\n",
      "Processing frame 165\n",
      "Processing frame 166\n",
      "Processing frame 167\n",
      "Processing frame 168\n",
      "Processing frame 169\n",
      "Processing frame 170\n",
      "Processing frame 171\n",
      "Processing frame 172\n",
      "Processing frame 173\n",
      "Processing frame 174\n",
      "Processing frame 175\n",
      "Processing frame 176\n",
      "Processing frame 177\n",
      "Processing frame 178\n",
      "Processing frame 179\n",
      "Processing frame 180\n",
      "Processing frame 181\n",
      "Processing frame 182\n",
      "Processing frame 183\n",
      "Processing frame 184\n",
      "Processing frame 185\n",
      "Processing frame 186\n",
      "Processing frame 187\n",
      "Processing frame 188\n",
      "Processing frame 189\n",
      "Processing frame 190\n",
      "Processing frame 191\n",
      "Processing frame 192\n",
      "Processing frame 193\n",
      "Processing frame 194\n",
      "Processing frame 195\n",
      "Processing frame 196\n",
      "Processing frame 197\n",
      "Processing frame 198\n",
      "Processing frame 199\n",
      "Processing frame 200\n",
      "Processing frame 201\n",
      "Processing frame 202\n",
      "Processing frame 203\n",
      "Processing frame 204\n",
      "Processing frame 205\n",
      "Processing frame 206\n",
      "Processing frame 207\n",
      "Processing frame 208\n",
      "Processing frame 209\n",
      "Processing frame 210\n",
      "Processing frame 211\n",
      "Processing frame 212\n",
      "Processing frame 213\n",
      "Processing frame 214\n",
      "Processing frame 215\n",
      "Processing frame 216\n",
      "Processed 216 frames\n",
      "Average FPS: 29.94\n",
      "Output saved to: output\\video (12)_fall_detection.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from collections import deque\n",
    "from utils.datasets import letterbox\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from utils.general import non_max_suppression_kpt, strip_optimizer\n",
    "from torchvision import transforms\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, poseweights='yolov7-w6-pose.pt', device='0'):\n",
    "        \"\"\"\n",
    "        Initialize the Fall Detector with parameters as defined in the paper\n",
    "        \"Enhanced Fall Detection Using YOLOv7-W6-Pose for Real-Time Elderly Monitoring\"\n",
    "        \n",
    "        Key parameters:\n",
    "        - LENGTH_FACTOR_ALPHA (α): Used in height condition formula (Section 3.1)\n",
    "        - VELOCITY_THRESHOLD: Threshold for fall speed detection (Section 3.2)\n",
    "        - LEG_ANGLE_THRESHOLD: Degrees threshold for leg angles (Section 3.2)\n",
    "        - TORSO_ANGLE_THRESHOLD: Degrees threshold for torso orientation (Section 3.2)\n",
    "        - ASPECT_RATIO_THRESHOLD: Width/height ratio threshold (Section 3.1)\n",
    "        - CONFIDENCE_THRESHOLD: Minimum keypoint confidence for reliable detection\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Fall Detector with weights: {poseweights} on device: {device}\")\n",
    "        \n",
    "        # Select the appropriate device\n",
    "        self.device = select_device(device)\n",
    "        self.half = self.device.type != 'cpu'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = attempt_load(poseweights, map_location=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        \n",
    "        # Threshold parameters as defined in the paper\n",
    "        self.LENGTH_FACTOR_ALPHA = 0.5  # α in the height condition formula\n",
    "        self.VELOCITY_THRESHOLD = 1.0    # px/frame for fall speed\n",
    "        self.LEG_ANGLE_THRESHOLD = 45    # degrees for leg angles\n",
    "        self.TORSO_ANGLE_THRESHOLD = 50  # degrees for torso orientation\n",
    "        self.ASPECT_RATIO_THRESHOLD = 0.8 # width/height ratio\n",
    "        self.CONFIDENCE_THRESHOLD = 0.4  # minimum keypoint confidence\n",
    "        self.TARGET_FPS = 25\n",
    "        \n",
    "        # State tracking variables\n",
    "        self.prev_keypoints = None\n",
    "        self.velocity_buffer = deque(maxlen=3)  # tracks vertical speed\n",
    "        self.fall_buffer = deque(maxlen=2)      # confirmation buffer\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.prev_shoulder_y = None\n",
    "        \n",
    "        # Fall detection status\n",
    "        self.fall_detected = False\n",
    "    \n",
    "    def calculate_euclidean_distance(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between two points\n",
    "        Used in the paper to measure distances between key body points,\n",
    "        particularly for the Lfactor (length factor) calculation in Section 3.1\n",
    "        \n",
    "        Args:\n",
    "            point1, point2: Coordinate points (x,y)\n",
    "        Returns:\n",
    "            Euclidean distance between the points\n",
    "        \"\"\"\n",
    "        return math.hypot(point1[0]-point2[0], point1[1]-point2[1])\n",
    "\n",
    "    def calculate_angle(self, a, b, c):\n",
    "        \"\"\"\n",
    "        Calculate angle between three points (in degrees)\n",
    "        Used in the paper for calculating leg angles (Section 3.2)\n",
    "        \n",
    "        Args:\n",
    "            a, b, c: Three points where b is the vertex\n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ba = np.array([a[0]-b[0], a[1]-b[1]])\n",
    "            bc = np.array([c[0]-b[0], c[1]-b[1]])\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "            return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "        except:\n",
    "            return 180  # return maximum angle if calculation fails\n",
    "\n",
    "    def calculate_torso_angle(self, shoulders, hips):\n",
    "        \"\"\"\n",
    "        Calculate torso angle relative to vertical axis\n",
    "        Implements the torso orientation assessment described in Section 3.2\n",
    "        of the paper to detect when the torso is horizontal (fallen state)\n",
    "        \n",
    "        Args:\n",
    "            shoulders: list of shoulder points [(x,y), (x,y)]\n",
    "            hips: list of hip points [(x,y), (x,y)]\n",
    "        Returns:\n",
    "            angle in degrees between torso and vertical axis\n",
    "        \"\"\"\n",
    "        shoulder_center = np.mean(shoulders, axis=0)\n",
    "        hip_center = np.mean(hips, axis=0)\n",
    "        vertical_vector = np.array([0, 1])\n",
    "        torso_vector = np.array([hip_center[0]-shoulder_center[0], \n",
    "                                hip_center[1]-shoulder_center[1]])\n",
    "        \n",
    "        if np.linalg.norm(torso_vector) < 1e-6:\n",
    "            return 90  # neutral angle if points overlap\n",
    "            \n",
    "        cosine = np.dot(torso_vector, vertical_vector) / (np.linalg.norm(torso_vector) + 1e-6)\n",
    "        return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"\n",
    "        Main fall detection function implementing the paper's algorithm from Sections 3.1 and 3.2\n",
    "        Combines multiple conditions (height, velocity, angles, aspect ratio) to detect falls\n",
    "        \n",
    "        Args:\n",
    "            keypoints: Array of 17 keypoints with (x,y,confidence)\n",
    "        Returns:\n",
    "            tuple: (is_fall, state, condition_info)\n",
    "        \"\"\"\n",
    "        # Keypoint indices as defined in the paper\n",
    "        NOSE = 0\n",
    "        LEFT_SHOULDER = 5\n",
    "        RIGHT_SHOULDER = 6\n",
    "        LEFT_HIP = 11\n",
    "        RIGHT_HIP = 12\n",
    "        LEFT_KNEE = 13\n",
    "        RIGHT_KNEE = 14\n",
    "        LEFT_ANKLE = 15\n",
    "        RIGHT_ANKLE = 16\n",
    "        \n",
    "        try:\n",
    "            # Extract keypoints with confidence check\n",
    "            kp = {}\n",
    "            \n",
    "            # Reshape keypoints to get (x, y, conf) format for each keypoint\n",
    "            reshaped_kpts = keypoints.reshape(-1, 3)\n",
    "            \n",
    "            # Extract specific keypoints\n",
    "            kp['nose'] = reshaped_kpts[NOSE]\n",
    "            kp['left_shoulder'] = reshaped_kpts[LEFT_SHOULDER]\n",
    "            kp['right_shoulder'] = reshaped_kpts[RIGHT_SHOULDER]\n",
    "            kp['left_hip'] = reshaped_kpts[LEFT_HIP]\n",
    "            kp['right_hip'] = reshaped_kpts[RIGHT_HIP]\n",
    "            kp['left_knee'] = reshaped_kpts[LEFT_KNEE]\n",
    "            kp['right_knee'] = reshaped_kpts[RIGHT_KNEE]\n",
    "            kp['left_ankle'] = reshaped_kpts[LEFT_ANKLE]\n",
    "            kp['right_ankle'] = reshaped_kpts[RIGHT_ANKLE]\n",
    "            \n",
    "            # Confidence check for all keypoints\n",
    "            if any(point[2] < self.CONFIDENCE_THRESHOLD for point in kp.values()):\n",
    "                return False, \"low_confidence\", []\n",
    "\n",
    "            # Get coordinates (convert to tuples for clarity)\n",
    "            ls = (kp['left_shoulder'][0], kp['left_shoulder'][1])\n",
    "            rs = (kp['right_shoulder'][0], kp['right_shoulder'][1])\n",
    "            lh = (kp['left_hip'][0], kp['left_hip'][1])\n",
    "            rh = (kp['right_hip'][0], kp['right_hip'][1])\n",
    "            lk = (kp['left_knee'][0], kp['left_knee'][1])\n",
    "            rk = (kp['right_knee'][0], kp['right_knee'][1])\n",
    "            la = (kp['left_ankle'][0], kp['left_ankle'][1])\n",
    "            ra = (kp['right_ankle'][0], kp['right_ankle'][1])\n",
    "\n",
    "            \"\"\" 1. HEIGHT CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Calculate length factor (Lfactor) as Euclidean distance\n",
    "            torso_mid = ((lh[0] + rh[0])/2, (lh[1] + rh[1])/2)\n",
    "            Lfactor = self.calculate_euclidean_distance(ls, torso_mid)\n",
    "            \n",
    "            # Get vertical positions\n",
    "            max_feet_y = max(la[1], ra[1])\n",
    "            min_shoulder_y = min(ls[1], rs[1])\n",
    "            \n",
    "            # Paper's height condition: yl ≤ yFl + α·Lfactor\n",
    "            height_cond = min_shoulder_y >= (max_feet_y - self.LENGTH_FACTOR_ALPHA * Lfactor)\n",
    "            \n",
    "            \"\"\" 2. VELOCITY CONDITION (Paper Section 3.2) \"\"\"\n",
    "            current_time = time.time()\n",
    "            vertical_speed = 0\n",
    "            current_min_y = min(ls[1], rs[1])\n",
    "            \n",
    "            if self.prev_shoulder_y is not None and self.prev_frame_time is not None:\n",
    "                time_elapsed = current_time - self.prev_frame_time\n",
    "                if time_elapsed > 0:\n",
    "                    vertical_speed = (current_min_y - self.prev_shoulder_y) / time_elapsed\n",
    "                    self.velocity_buffer.append(abs(vertical_speed))\n",
    "            \n",
    "            avg_speed = sum(self.velocity_buffer)/len(self.velocity_buffer) if self.velocity_buffer else 0\n",
    "            speed_cond = avg_speed >= self.VELOCITY_THRESHOLD\n",
    "            \n",
    "            \"\"\" 3. ANGLE CONDITIONS (Paper Section 3.2) \"\"\"\n",
    "            left_leg_angle = self.calculate_angle(lh, lk, la)\n",
    "            right_leg_angle = self.calculate_angle(rh, rk, ra)\n",
    "            leg_angle_cond = min(left_leg_angle, right_leg_angle) < self.LEG_ANGLE_THRESHOLD\n",
    "            \n",
    "            # Torso orientation (not explicitly in paper but mentioned in text)\n",
    "            torso_angle = self.calculate_torso_angle([ls, rs], [lh, rh])\n",
    "            torso_cond = torso_angle > self.TORSO_ANGLE_THRESHOLD\n",
    "            \n",
    "            \"\"\" 4. ASPECT RATIO CONDITION (Paper Section 3.1) \"\"\"\n",
    "            # Body orientation ratio: width/height\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            head_to_feet = abs(kp['nose'][1] - max_feet_y)\n",
    "            orientation_ratio = body_width / (head_to_feet + 1e-6)\n",
    "            aspect_cond = orientation_ratio > self.ASPECT_RATIO_THRESHOLD\n",
    "            \n",
    "            \"\"\" FALL DECISION LOGIC (Paper Section 3) \"\"\"\n",
    "            # Combined conditions - at least 2 must be true\n",
    "            conditions_met = sum([height_cond, speed_cond, leg_angle_cond, torso_cond, aspect_cond])\n",
    "            \n",
    "            # State determination\n",
    "            current_state = \"normal\"\n",
    "            conditions_info = []\n",
    "            \n",
    "            if height_cond:\n",
    "                if speed_cond:  # Rapid descent\n",
    "                    current_state = \"falling\"\n",
    "                    self.fall_start_time = current_time\n",
    "                    conditions_info.append(f\"speed:{avg_speed:.1f}px/s\")\n",
    "                elif torso_cond and self.fall_start_time and (current_time - self.fall_start_time < 1.0):\n",
    "                    current_state = \"fallen\"\n",
    "                    conditions_info.append(\"horizontal\")\n",
    "            \n",
    "            if leg_angle_cond:\n",
    "                conditions_info.append(f\"leg_angle:{min(left_leg_angle, right_leg_angle):.0f}°\")\n",
    "            \n",
    "            if aspect_cond:\n",
    "                conditions_info.append(f\"aspect:{orientation_ratio:.2f}\")\n",
    "            \n",
    "            # Final decision with confirmation buffer\n",
    "            is_fall = conditions_met >= 2\n",
    "            self.fall_buffer.append(is_fall)\n",
    "            final_detection = sum(self.fall_buffer) >= 2 if len(self.fall_buffer) >= 1 else is_fall\n",
    "            \n",
    "            if final_detection:\n",
    "                current_state = \"fallen\"\n",
    "                self.fall_detected = True\n",
    "            else:\n",
    "                self.fall_detected = False\n",
    "            \n",
    "            # Update tracking variables\n",
    "            self.prev_keypoints = kp\n",
    "            self.prev_shoulder_y = current_min_y\n",
    "            self.prev_frame_time = current_time\n",
    "            \n",
    "            # Diagnostic information\n",
    "            conditions_info.extend([\n",
    "                f\"height:{'Y' if height_cond else 'N'}\",\n",
    "                f\"speed:{'Y' if speed_cond else 'N'}\",\n",
    "                f\"leg_angle:{'Y' if leg_angle_cond else 'N'}\",\n",
    "                f\"torso:{'Y' if torso_cond else 'N'}\",\n",
    "                f\"aspect:{'Y' if aspect_cond else 'N'}\",\n",
    "                f\"conf:{min(p[2] for p in kp.values()):.2f}\"\n",
    "            ])\n",
    "            \n",
    "            return final_detection, current_state, conditions_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Detection error: {str(e)}\")\n",
    "            return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for fall detection\n",
    "        \n",
    "        Args:\n",
    "            frame: Video frame to process\n",
    "            \n",
    "        Returns:\n",
    "            frame: Processed frame with detections\n",
    "            is_fall: Boolean indicating whether a fall was detected\n",
    "            state: Current state (normal, falling, fallen)\n",
    "            condition_info: List of conditions that triggered the detection\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        orig_image = frame.copy()\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image while maintaining aspect ratio\n",
    "        frame_height, frame_width = orig_image.shape[:2]\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        image = image.float()\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(image)\n",
    "            \n",
    "        # Post-process\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=self.model.yaml['nc'], nkpt=self.model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        \n",
    "        # Convert back to BGR for display\n",
    "        img = image[0].permute(1, 2, 0) * 255\n",
    "        img = img.cpu().numpy().astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize fall status and state for this frame\n",
    "        is_fall = False\n",
    "        current_state = \"normal\"\n",
    "        condition_info = []\n",
    "        \n",
    "        # Process each person detected\n",
    "        for idx in range(output.shape[0]):\n",
    "            # Draw skeleton and keypoints\n",
    "            plot_skeleton_kpts(img, output[idx, 7:].T, 3)\n",
    "            \n",
    "            # Calculate improved bounding box based on keypoints (YouTube approach)\n",
    "            # Find the minimum and maximum x,y coordinates from all keypoints\n",
    "            kpts = output[idx, 7:].reshape(-1, 3)\n",
    "            \n",
    "            # Initialize with first keypoint\n",
    "            x_values = [kpt[0] for kpt in kpts if kpt[2] > 0.5]  # Only use keypoints with confidence > 0.5\n",
    "            y_values = [kpt[1] for kpt in kpts if kpt[2] > 0.5]\n",
    "            \n",
    "            if x_values and y_values:  # Check if we have valid keypoints\n",
    "                xmin, ymin = min(x_values), min(y_values)\n",
    "                xmax, ymax = max(x_values), max(y_values)\n",
    "                \n",
    "                # Add padding to make bounding box a bit larger\n",
    "                padding = 10\n",
    "                xmin = max(0, xmin - padding)\n",
    "                ymin = max(0, ymin - padding)\n",
    "                xmax = xmax + padding\n",
    "                ymax = ymax + padding\n",
    "                \n",
    "                # Calculate aspect ratio for reference (not used in detection)\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                bbox_aspect_ratio = width / height if height > 0 else 0\n",
    "                \n",
    "                # Calculate center\n",
    "                cx = int((xmin + xmax) // 2)\n",
    "                cy = int((ymin + ymax) // 2)\n",
    "                \n",
    "                # For debugging: show aspect ratio on frame\n",
    "                cv2.putText(img, f\"Ratio: {bbox_aspect_ratio:.2f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                # Fallback to original bounding box if no valid keypoints\n",
    "                x1, y1, x2, y2 = output[idx, 0], output[idx, 1], output[idx, 2], output[idx, 3]\n",
    "                xmin, ymin = x1, y1\n",
    "                xmax, ymax = x2, y2\n",
    "                cx, cy = int((x1 + x2) // 2), int((y1 + y2) // 2)\n",
    "            \n",
    "            # Get key points for this person\n",
    "            key_points = output[idx, 7:]\n",
    "            \n",
    "            # Detect fall for this person using enhanced algorithm\n",
    "            person_fall, person_state, person_conditions = self.detect_fall(key_points)\n",
    "            \n",
    "            # If any person is falling, set global fall status\n",
    "            if person_fall:\n",
    "                is_fall = True\n",
    "                current_state = person_state\n",
    "                condition_info = person_conditions\n",
    "                \n",
    "                # Add visual indication of fall\n",
    "                status_text = f\"FALL DETECTED: {person_state.upper()}\"\n",
    "                cv2.putText(img, status_text, (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Draw the bounding box in red for a fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255), 2)\n",
    "                \n",
    "                # For YouTube-style visual, add a colored rectangle at the center\n",
    "                cv2.rectangle(img, (cx-10, cy-10), (cx+10, cy+10), (84, 61, 247), -1)\n",
    "                \n",
    "                # Add condition info to the frame\n",
    "                for i, cond in enumerate(person_conditions[:3]):  # Show first 3 conditions only\n",
    "                    cv2.putText(img, cond, (10, 60 + i*25), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)\n",
    "            else:\n",
    "                # Draw normal bounding box in green for no fall\n",
    "                cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 1)\n",
    "                \n",
    "                # Show normal state\n",
    "                cv2.putText(img, f\"State: {person_state}\", (10, 60), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return img, is_fall, current_state, condition_info\n",
    "\n",
    "def run_fall_detection(poseweights='yolov7-w6-pose.pt', source='pose.mp4', device='cpu', display=True, save_output=True):\n",
    "    \"\"\"\n",
    "    Run fall detection on a video or webcam feed\n",
    "    \n",
    "    Args:\n",
    "        poseweights: Path to the YOLOv7 pose weights\n",
    "        source: Path to video file or webcam ID (0, 1, etc.)\n",
    "        device: Device to run inference on ('cpu' or '0', '1', etc. for GPU)\n",
    "        display: Whether to show video with detections in real-time\n",
    "        save_output: Whether to save the output video\n",
    "    \"\"\"\n",
    "    # Initialize the fall detector\n",
    "    detector = FallDetector(poseweights=poseweights, device=device)\n",
    "    \n",
    "    # Parse the input source\n",
    "    input_path = source\n",
    "    if source.isnumeric():\n",
    "        input_path = int(source)\n",
    "    \n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Setup output video writer if requested\n",
    "    out = None\n",
    "    if save_output:\n",
    "        if isinstance(input_path, int):\n",
    "            # For webcam\n",
    "            output_path = os.path.join('output', f\"webcam_fall_detection.mp4\")\n",
    "        else:\n",
    "            # For video file\n",
    "            filename = os.path.basename(input_path).split('.')[0]\n",
    "            output_path = os.path.join('output', f\"{filename}_fall_detection.mp4\")\n",
    "        \n",
    "        # Create VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    total_fps = 0\n",
    "    \n",
    "    print(\"Starting fall detection...\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}\")\n",
    "        \n",
    "        # Process frame for fall detection\n",
    "        start_time = time.time()\n",
    "        processed_frame, is_fall, current_state, condition_info = detector.process_frame(frame)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate FPS\n",
    "        processing_fps = 1 / (end_time - start_time)\n",
    "        total_fps += processing_fps\n",
    "        \n",
    "        # Resize processed frame to match original dimensions for display and saving\n",
    "        processed_frame_resized = cv2.resize(processed_frame, (frame_width, frame_height))\n",
    "        \n",
    "        # Add FPS info\n",
    "        cv2.putText(processed_frame_resized, f\"FPS: {processing_fps:.2f}\", (frame_width - 150, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame if requested\n",
    "        if display:\n",
    "            cv2.imshow('Fall Detection', processed_frame_resized)\n",
    "            \n",
    "            # Exit on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Save frame to output video if requested\n",
    "        if save_output and out is not None:\n",
    "            out.write(processed_frame_resized)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if save_output and out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print statistics\n",
    "    if frame_count > 0:\n",
    "        avg_fps = total_fps / frame_count\n",
    "        print(f\"Processed {frame_count} frames\")\n",
    "        print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "        if save_output:\n",
    "            print(f\"Output saved to: {output_path}\")\n",
    "\n",
    "def process_dataset_folder(dataset_path, detector, is_sorted=False):\n",
    "    \"\"\"\n",
    "    Process the Le2i fall detection dataset with the hierarchical folder structure\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the dataset root directory\n",
    "        detector: Initialized FallDetector instance\n",
    "        is_sorted: Boolean indicating if the dataset is in sorted structure (Fall/Non Fall folders)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import cv2\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    # Path to Le2i dataset\n",
    "    le2i_path = os.path.join(dataset_path, \"le2i\")\n",
    "    \n",
    "    # Initialize tracking variables for performance metrics\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Total counters for statistics\n",
    "    total_videos = 0\n",
    "    total_frames = 0\n",
    "    processing_times = []\n",
    "    \n",
    "    print(f\"Starting batch processing of Le2i dataset at {le2i_path}\")\n",
    "    \n",
    "    if is_sorted:\n",
    "        # Process the sorted structure (Fall/Non Fall folders)\n",
    "        fall_folder = os.path.join(le2i_path, \"Le2i_Sorted\", \"Fall\")\n",
    "        nonfall_folder = os.path.join(le2i_path, \"Le2i_Sorted\", \"Non Fall\")  # Corrected folder name with space\n",
    "        \n",
    "        # Check if Fall folder exists\n",
    "        if os.path.exists(fall_folder):\n",
    "            print(f\"Processing Fall folder: {fall_folder}\")\n",
    "            \n",
    "            # Get all environment subfolders\n",
    "            env_folders = [d for d in os.listdir(fall_folder) if os.path.isdir(os.path.join(fall_folder, d))]\n",
    "            print(f\"Found {len(env_folders)} environment folders in Fall folder: {', '.join(env_folders)}\")\n",
    "            \n",
    "            # Process each environment folder\n",
    "            for env_folder in env_folders:\n",
    "                env_path = os.path.join(fall_folder, env_folder)\n",
    "                videos_folder = os.path.join(env_path, \"Videos\")\n",
    "                \n",
    "                if os.path.exists(videos_folder):\n",
    "                    print(f\"Processing videos in {env_folder}/Videos\")\n",
    "                    \n",
    "                    # Get all video files\n",
    "                    video_files = [f for f in os.listdir(videos_folder) \n",
    "                                  if f.endswith(('.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV'))]\n",
    "                    \n",
    "                    print(f\"Found {len(video_files)} video files in {env_folder}/Videos\")\n",
    "                    \n",
    "                    for video_file in video_files:\n",
    "                        video_path = os.path.join(videos_folder, video_file)\n",
    "                        print(f\"Processing fall video: {env_folder}/{video_file}\")\n",
    "                        \n",
    "                        # Extract true labels (1 for fall)\n",
    "                        true_label = 1\n",
    "                        \n",
    "                        try:\n",
    "                            # Process the video and get predictions\n",
    "                            result, frames_processed, avg_time = process_single_video(video_path, detector, true_label)\n",
    "                            \n",
    "                            # Extend the lists with results\n",
    "                            true_labels.extend([true_label] * len(result))\n",
    "                            predicted_labels.extend(result)\n",
    "                            \n",
    "                            # Update statistics\n",
    "                            total_videos += 1\n",
    "                            total_frames += frames_processed\n",
    "                            processing_times.append(avg_time)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing video {video_file}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"Videos folder not found in {env_folder}\")\n",
    "        else:\n",
    "            print(f\"Fall folder not found at {fall_folder}\")\n",
    "        \n",
    "        # Check if NonFall folder exists\n",
    "        if os.path.exists(nonfall_folder):\n",
    "            print(f\"Processing Non Fall folder: {nonfall_folder}\")\n",
    "            \n",
    "            # Get all environment subfolders\n",
    "            env_folders = [d for d in os.listdir(nonfall_folder) if os.path.isdir(os.path.join(nonfall_folder, d))]\n",
    "            print(f\"Found {len(env_folders)} environment folders in Non Fall folder: {', '.join(env_folders)}\")\n",
    "            \n",
    "            # Process each environment folder\n",
    "            for env_folder in env_folders:\n",
    "                env_path = os.path.join(nonfall_folder, env_folder)\n",
    "                videos_folder = os.path.join(env_path, \"Videos\")\n",
    "                \n",
    "                if os.path.exists(videos_folder):\n",
    "                    print(f\"Processing videos in {env_folder}/Videos\")\n",
    "                    \n",
    "                    # Get all video files\n",
    "                    video_files = [f for f in os.listdir(videos_folder) \n",
    "                                  if f.endswith(('.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV'))]\n",
    "                    \n",
    "                    print(f\"Found {len(video_files)} video files in {env_folder}/Videos\")\n",
    "                    \n",
    "                    for video_file in video_files:\n",
    "                        video_path = os.path.join(videos_folder, video_file)\n",
    "                        print(f\"Processing non-fall video: {env_folder}/{video_file}\")\n",
    "                        \n",
    "                        # Extract true labels (0 for non-fall)\n",
    "                        true_label = 0\n",
    "                        \n",
    "                        try:\n",
    "                            # Process the video and get predictions\n",
    "                            result, frames_processed, avg_time = process_single_video(video_path, detector, true_label)\n",
    "                            \n",
    "                            # Extend the lists with results\n",
    "                            true_labels.extend([true_label] * len(result))\n",
    "                            predicted_labels.extend(result)\n",
    "                            \n",
    "                            # Update statistics\n",
    "                            total_videos += 1\n",
    "                            total_frames += frames_processed\n",
    "                            processing_times.append(avg_time)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing video {video_file}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"Videos folder not found in {env_folder}\")\n",
    "        else:\n",
    "            print(f\"Non Fall folder not found at {nonfall_folder}\")\n",
    "    else:\n",
    "        # Process the traditional structure with mixed videos and ground truth files\n",
    "        videos_folder = os.path.join(le2i_path, \"Videos\")\n",
    "        \n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(videos_folder):\n",
    "            print(f\"Error: Videos directory not found at {videos_folder}\")\n",
    "            return\n",
    "            \n",
    "        # List of environments in Le2i dataset\n",
    "        environments = [\"Home\", \"Coffee_room\", \"Office\", \"Lecture_room\"]\n",
    "        \n",
    "        for env in environments:\n",
    "            env_folder = os.path.join(videos_folder, env)\n",
    "            if os.path.exists(env_folder):\n",
    "                print(f\"Processing environment: {env}\")\n",
    "                \n",
    "                # Get all video files\n",
    "                video_files = [f for f in os.listdir(env_folder) if f.endswith(('.mp4', '.avi'))]\n",
    "                \n",
    "                for video_file in video_files:\n",
    "                    video_path = os.path.join(env_folder, video_file)\n",
    "                    print(f\"Processing video: {video_file}\")\n",
    "                    \n",
    "                    # Determine if this is a fall video based on filename\n",
    "                    # In Le2i dataset, videos with 'fall' in the name are fall videos\n",
    "                    true_label = 1 if 'fall' in video_file.lower() else 0\n",
    "                    \n",
    "                    # Process the video and get predictions\n",
    "                    result, frames_processed, avg_time = process_single_video(video_path, detector, true_label)\n",
    "                    \n",
    "                    # Extend the lists with results\n",
    "                    true_labels.extend([true_label] * len(result))\n",
    "                    predicted_labels.extend(result)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_videos += 1\n",
    "                    total_frames += frames_processed\n",
    "                    processing_times.append(avg_time)\n",
    "    \n",
    "    # Calculate and display performance metrics based on paper's key metrics\n",
    "    if true_labels and predicted_labels:\n",
    "        calculate_and_display_metrics(true_labels, predicted_labels)\n",
    "    \n",
    "    # Display overall processing statistics\n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {total_videos}\")\n",
    "    print(f\"Total frames processed: {total_frames}\")\n",
    "    if processing_times:\n",
    "        avg_processing_time = sum(processing_times) / len(processing_times)\n",
    "        print(f\"Average processing time per frame: {avg_processing_time:.4f} seconds\")\n",
    "        print(f\"Average FPS: {1/avg_processing_time:.2f}\")\n",
    "\n",
    "\n",
    "def process_single_video(video_path, detector, true_label):\n",
    "    \"\"\"\n",
    "    Process a single video file and return predictions\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        detector: Initialized FallDetector instance\n",
    "        true_label: Ground truth label (1 for fall, 0 for non-fall)\n",
    "        \n",
    "    Returns:\n",
    "        predictions: List of predicted labels for each frame\n",
    "        frames_processed: Number of frames processed\n",
    "        avg_processing_time: Average processing time per frame\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    print(f\"Opening video file: {video_path}\")\n",
    "    # Verify the file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file does not exist: {video_path}\")\n",
    "        return [0], 0, 0\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        print(f\"Video file exists: {os.path.exists(video_path)}\")\n",
    "        print(f\"Video file size: {os.path.getsize(video_path) if os.path.exists(video_path) else 'N/A'} bytes\")\n",
    "        return [0], 0, 0\n",
    "    \n",
    "    # Get video properties for debugging\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    # Initialize counters and results\n",
    "    frames_processed = 0\n",
    "    predictions = []\n",
    "    processing_times = []\n",
    "    \n",
    "    # Process each frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frames_processed += 1\n",
    "        \n",
    "        # Process frame for fall detection\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            processed_frame, is_fall, current_state, condition_info = detector.process_frame(frame)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            processing_time = end_time - start_time\n",
    "            processing_times.append(processing_time)\n",
    "            \n",
    "            # Record prediction (1 for fall, 0 for non-fall)\n",
    "            predictions.append(1 if is_fall else 0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frames_processed}: {str(e)}\")\n",
    "            predictions.append(0)  # Default to no fall on error\n",
    "        \n",
    "        # Show progress every 5 frames\n",
    "        if frames_processed % 5 == 0:\n",
    "            print(f\"Processed {frames_processed}/{total_frames} frames from {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # Release video capture\n",
    "    cap.release()\n",
    "    \n",
    "    # Check if we processed any frames\n",
    "    if not processing_times:\n",
    "        print(\"No frames were successfully processed.\")\n",
    "        return [0], 0, 0\n",
    "    \n",
    "    # Calculate average processing time\n",
    "    avg_processing_time = sum(processing_times) / len(processing_times)\n",
    "    \n",
    "    # Handle predictions - use majority voting to determine overall video prediction\n",
    "    # For fall videos (true_label = 1), even a single fall detection is considered a fall\n",
    "    # For non-fall videos (true_label = 0), even a single false fall detection is considered a false positive\n",
    "    if true_label == 1:  \n",
    "        video_prediction = 1 if 1 in predictions else 0\n",
    "    else:  \n",
    "        video_prediction = 1 if 1 in predictions else 0\n",
    "    \n",
    "    print(f\"Video prediction: {video_prediction} (True label: {true_label})\")\n",
    "    print(f\"Processed {frames_processed} frames with avg processing time: {avg_processing_time:.4f} seconds\")\n",
    "    \n",
    "    return [video_prediction], frames_processed, avg_processing_time\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate and display performance metrics based on the paper's key metrics\n",
    "    \n",
    "    Args:\n",
    "        true_labels: List of ground truth labels\n",
    "        predicted_labels: List of predicted labels\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check if there are enough labels to calculate metrics\n",
    "    if not true_labels or not predicted_labels:\n",
    "        print(\"No data available to calculate metrics.\")\n",
    "        return\n",
    "    \n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        print(f\"Error: Mismatch in label lengths. True: {len(true_labels)}, Predicted: {len(predicted_labels)}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    try:\n",
    "        cm = confusion_matrix(true_labels, predicted_labels, labels=[0, 1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating confusion matrix: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate metrics based on the paper\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Display confusion matrix in a more readable format\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    \n",
    "    # Display metrics with percentage format matching the paper\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.2%}\")\n",
    "    print(f\"Specificity: {specificity:.2%}\")\n",
    "    print(f\"F1 Score: {f1:.2%}\")\n",
    "    \n",
    "    # Additional comparison with paper's metrics\n",
    "    print(\"\\nComparison with Paper's Metrics:\")\n",
    "    print(\"                  | Our Method | Paper's Method\")\n",
    "    print(\"------------------|------------|---------------\")\n",
    "    print(f\"Accuracy          | {accuracy:.2%}     | 96.15%\")\n",
    "    print(f\"Precision         | {precision:.2%}     | 97.00%\")\n",
    "    print(f\"Recall            | {recall:.2%}     | 97.98%\")\n",
    "    print(f\"Specificity       | {specificity:.2%}     | 90.32%\")\n",
    "    print(f\"F1 Score          | {f1:.2%}     | 97.48%\")\n",
    "\n",
    "def run_interactive():\n",
    "    \"\"\"\n",
    "    Interactive function to run fall detection with user input\n",
    "    \"\"\"\n",
    "    # Get the weights file\n",
    "    poseweights = input(\"Enter path to weights file [default: yolov7-w6-pose.pt]: \") or \"yolov7-w6-pose.pt\"\n",
    "    \n",
    "    # Get device type\n",
    "    use_gpu = input(\"Use GPU? (y/n) [default: y]: \").lower() or \"y\"\n",
    "    if use_gpu == \"y\":\n",
    "        device = input(\"Enter GPU device ID [default: 0]: \") or \"0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Get source type\n",
    "    print(\"\\nSelect input source:\")\n",
    "    print(\"1: Video file\")\n",
    "    print(\"2: Webcam\")\n",
    "    print(\"3: Batch process (Le2i dataset)\")\n",
    "    source_choice = input(\"Enter choice [1/2/3]: \")\n",
    "    \n",
    "    if source_choice == \"1\":\n",
    "        # Video file\n",
    "        default_video = \"sample_video.mp4\"\n",
    "        source = input(f\"Enter video file path [default: {default_video}]: \") or default_video\n",
    "        # Ask if user wants to display the processed video in real-time\n",
    "        display_video = input(\"Display video with pose estimation in real-time? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        # Ask if user wants to save the output video\n",
    "        save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        print(f\"\\nRunning fall detection with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Source: {source}\")\n",
    "        print(f\"- Display: {'Yes' if display_video == 'y' else 'No'}\")\n",
    "        print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # Run the model\n",
    "            run_with_display = (display_video == \"y\")\n",
    "            save_output = (save_video == \"y\")\n",
    "            \n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Run fall detection\n",
    "            run_fall_detection(\n",
    "                poseweights=poseweights,\n",
    "                source=source,\n",
    "                device=device,\n",
    "                display=run_with_display,\n",
    "                save_output=save_output\n",
    "            )\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    elif source_choice == \"2\":\n",
    "        # Webcam\n",
    "        cam_id = input(\"Enter webcam ID [default: 0]: \") or \"0\"\n",
    "        source = cam_id\n",
    "        \n",
    "        # Ask if user wants to save the output video\n",
    "        save_video = input(\"Save output video? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        print(f\"\\nRunning fall detection with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Source: Webcam {source}\")\n",
    "        print(f\"- Display: Yes\")  # Always display for webcam\n",
    "        print(f\"- Save output: {'Yes' if save_video == 'y' else 'No'}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # First strip optimizer to ensure model works correctly\n",
    "            strip_optimizer(device, poseweights)\n",
    "            \n",
    "            # Run fall detection\n",
    "            run_fall_detection(\n",
    "                poseweights=poseweights,\n",
    "                source=source,\n",
    "                device=device,\n",
    "                display=True,  # Always display for webcam\n",
    "                save_output=(save_video == \"y\")\n",
    "            )\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    elif source_choice == \"3\":\n",
    "        # Batch process (Le2i dataset)\n",
    "        default_dataset_path = \"datasets\"\n",
    "        dataset_path = input(f\"Enter dataset root path [default: {default_dataset_path}]: \") or default_dataset_path\n",
    "        \n",
    "        # Check if the dataset path exists\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: Dataset path '{dataset_path}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i dataset structure\n",
    "        le2i_path = os.path.join(dataset_path, \"le2i\")\n",
    "        if not os.path.exists(le2i_path):\n",
    "            print(f\"Error: Le2i dataset not found at {le2i_path}\")\n",
    "            return\n",
    "        \n",
    "        # Check for Le2i_Sorted structure\n",
    "        le2i_sorted_path = os.path.join(le2i_path, \"Le2i_Sorted\")\n",
    "        is_sorted = os.path.exists(le2i_sorted_path)\n",
    "        \n",
    "        if is_sorted:\n",
    "            print(f\"Found Le2i dataset with sorted structure (Fall/Non Fall folders)\")\n",
    "        else:\n",
    "            print(f\"Found Le2i dataset with traditional structure\")\n",
    "        \n",
    "        print(f\"\\nRunning batch processing with:\")\n",
    "        print(f\"- Weights: {poseweights}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Dataset path: {le2i_path}\")\n",
    "        print(f\"- Structure: {'Sorted' if is_sorted else 'Traditional'}\")\n",
    "        confirmation = input(\"\\nConfirm? (y/n) [default: y]: \").lower() or \"y\"\n",
    "        \n",
    "        if confirmation == \"y\":\n",
    "            # Initialize fall detector\n",
    "            detector = FallDetector(poseweights=poseweights, device=device)\n",
    "            \n",
    "            # Process the entire dataset\n",
    "            process_dataset_folder(dataset_path, detector, is_sorted)\n",
    "        else:\n",
    "            print(\"Operation cancelled\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Please run again and select a valid option.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run interactively\n",
    "    run_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cc128-d9e5-48a5-ab2e-d7477f45dbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
